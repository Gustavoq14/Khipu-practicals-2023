{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Jax, Optax, Haiku and Convolutional Neural Networks\n",
        "\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/jax_optax_haiku_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "## Copyright\n",
        "\n",
        "* Deep Learning Indaba 2022. Apache License 2.0.\n",
        "* Khipu 2023. Apache License 2.0.\n",
        "\n",
        "## Authors\n",
        "\n",
        "* Kale-ab Tessera (Indaba 2022)\n",
        "* Ignacio Ram√≠rez (Khipu 2023)\n",
        "\n",
        "## Reviewers\n",
        "* Javier Antoran\n",
        "* James Allingham\n",
        "* Ruan van der Merwe\n",
        "* Sebastian Bodenstein\n",
        "* Laurence Midgley\n",
        "* Joao Guilherme\n",
        "* Elan van Biljon\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "The first objective of this practical is to introduce you to the main tools used in the rest of the practicals. \n",
        "\n",
        "The second objective is to present and explain the mainstream architecture known as Convolutional Neural Networks (CNN), which is a key component in many Deep Learning architectures.\n",
        "\n",
        "## Topics\n",
        "\n",
        "* We begin by introducing **JAX**, a library that allows for very efficient speedups in numerical operations by transparently choosing the best available hardware (CPU, GPU, TPU, etc.) and provinding with a number of acceleration and automation mechanisms such as automatic differentiation (`grad`), parallelization (`pmap`), vectorization (`vmap`) and just-in-time compilation (`jit`).\n",
        "\n",
        "* The next step is [Optax](https://github.com/deepmind/optax) library: an efficient JAX-based optimization library.\n",
        "\n",
        "* The last library in the stack is [Haiku](https://github.com/deepmind/dm-haiku): an object-oriented library  for building and training neural networks based on JAX and Optax.\n",
        "\n",
        "* We then move on to Convolutional Neural Networks: a fundamental component in modern DL.\n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This practical assumes that you are familiar with:\n",
        "* Python and NumPy\n",
        "* Basic object oriented programming\n",
        "* Basic optimization (gradient descent, etc.)\n",
        "* Basic model learning (linear models)\n",
        "\n",
        "## Hardware requirements (**important**)\n",
        "\n",
        "For this practical, you will need to use a TPU or GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose either \"TPU\" (preferrable) or \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQe1CfDyrkdL"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "def plot_performance(data: Dict, title: str):\n",
        "    runs = list(data.keys())\n",
        "    time = list(data.values())\n",
        "\n",
        "    # creating the bar plot\n",
        "    plt.bar(runs, time, width=0.35)\n",
        "\n",
        "    plt.xlabel(\"Implementation\")\n",
        "    plt.ylabel(\"Average time taken (in s)\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "    best_perf_key = min(data, key=data.get)\n",
        "    all_runs_key = copy.copy(runs)\n",
        "\n",
        "    # all_runs_key_except_best\n",
        "    all_runs_key.remove(best_perf_key)\n",
        "\n",
        "    for k in all_runs_key:\n",
        "        print(\n",
        "            f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFzjRHUsUQqq"
      },
      "outputs": [],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\newcommand{\\because}[1]{&& \\triangleright \\textrm{#1}}\n",
        "$$"
      ],
      "metadata": {
        "id": "blMNBku0dB8h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbOEYsWQ6tHv"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 1 - JAX\n",
        "\n",
        "<hr>\n",
        "\n",
        "In a nutshell, JAX is an accelerator-based general purpose numerical computation library. Its basic API is similar to that of NumPy, making it easy to port already existing NumPy-based code to JAX.\n",
        "\n",
        "However, JAX has a key difference with NumPy (and many other similar libraries): it is based on the _functional programming_ paradigm. \n",
        "\n",
        "The basic postulate of functional programming is that functions are mappings from inputs to outputs, _without any side effects_. This means that, if you pass some object to a function, the object itself is never changed: a new object is returned. In fact, no other variable or inner state of the machine can be changed within the function.\n",
        "\n",
        "The above paradigm is extremely useful for generating efficient GPU/TPU-based programs, but makes some otherwise (traditional)  typical programming constructs inviable. We will see the benefits and caveats of all this below.\n",
        "\n",
        "\n",
        "## Similarities between JAX and NumPy \n",
        "\n",
        "The main similarity between JAX and NumPy is that they share a similar interface. In fact, often, JAX and NumPy arrays can be used interchangeably.The code below shows this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgRLq58OTz1t"
      },
      "outputs": [],
      "source": [
        "#@title NumPy\n",
        "\n",
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = np.linspace(-np.pi, np.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRQf2mNRTlt3"
      },
      "outputs": [],
      "source": [
        "#@title JAX\n",
        "# 100 linearly spaced numbers from -jnp.pi to jnp.pi\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=sin(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuNscwHeV_dn"
      },
      "source": [
        "**Exercise 1.1 - Code Task:** Can you plot the cosine function using `jnp`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5svZFPUCQNsG"
      },
      "outputs": [],
      "source": [
        "#@title Excercise 1.1\n",
        "# construct x, and y = cos(x) using JAX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4AVrGzy6JWR"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-jnp.pi, jnp.pi, 100)\n",
        "\n",
        "y = jnp.cos(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x, y, \"b\", label=\"y=cos(x)\")\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg4__l4A7yqc"
      },
      "source": [
        "## Differences between JAX and NumPy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      },
      "source": [
        "### JAX arrays are immutable, NumPy arrays are not\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). Allowing mutations makes transforms difficult and violates conditions for [pure functions](https://en.wikipedia.org/wiki/Pure_function).\n",
        "Let's see this in practice by changing the number at the beginning of an array. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NumPy\n",
        "\n"
      ],
      "metadata": {
        "id": "Vdfb1wtd-GkF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r-Los6YZR-f"
      },
      "outputs": [],
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX"
      ],
      "metadata": {
        "id": "8Y23OWjE_BDA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxjkKpqAZxWo"
      },
      "outputs": [],
      "source": [
        "# JAX: immutable arrays\n",
        "# Should raise an error.\n",
        "try:\n",
        "    x = jnp.arange(10)\n",
        "    x[0] = 10\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoWT5RBUagW8"
      },
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array. \n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJYxkh4qagwO"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(10)\n",
        "new_x = x.at[0].set(10)\n",
        "print(f\" new_x: {new_x} original x: {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut0meCGB5qD0"
      },
      "source": [
        "Note here that `new_x` is a copy and that the original `x` is unchanged. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAH4c_smdGQU"
      },
      "source": [
        "## Random number generation in NumPy and JAX \n",
        "\n",
        "A Pseudo Random Number Generator [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is an algorithm for producing a _deterministic_ sequence of numbers which _mimics_ the properties of a sequence of _random_ numbers.  \n",
        "\n",
        "All PRNGs have an internal state (sometimes also called a _key_, although these are not exactly the same). Because of the functional paradigm of JAX, this internal state needs to be handled more explicitly than `NumPy` (see `numpy.random`), `TensorFlow` or `PyTorch`).  \n",
        "\n",
        "Below we illustrate these differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2m376Ethf8m"
      },
      "source": [
        "## The NumPy way\n",
        "\n",
        "In Numpy, a PRNG is based on an internal `state`.\n",
        "The basic (non-recommended) NumPy method is to use a _global_ PRNG for all random-related operations (the actual recommended approach is to create an individual PRNG object for different tasks). \n",
        "\n",
        "As all PRNGs, we need to initialize its state with what is called a _seed_: typically an integer number  (suggested reading: \"The Hitchhiker's Guide to the Galaxy\" by Douglas Adams)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "# If you don't know why this is 42, read the Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "#\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKVz5atZMMOV"
      },
      "outputs": [],
      "source": [
        "# @title Helper function to compare PRNG keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1, prng_2):\n",
        "    \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "    # concat all elements in prng tuple\n",
        "    list_prng_data_equal = [(a == b) for a, b in zip(prng_1, prng_2)]\n",
        "    # stack all elements together\n",
        "    list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "    # check if all elements are the same\n",
        "    is_prng_equal = all(list_prng_data_equal)\n",
        "    return is_prng_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nloZ9abah3J3"
      },
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if PRNG keys/global state change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiUcfX7iSenY"
      },
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")\n",
        "prng_state = np.random.get_state()\n",
        "print(\n",
        "    f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHkW6V4iLa9"
      },
      "source": [
        "Numpy's global random state is updated every time a random number is generated, so *sample 1 != sample 2 != sample 3*. \n",
        "\n",
        "Having the state automatically updated makes it difficult to handle randomness in a **reproducible** way across different threads, processes and devices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGDU6ckKkzqL"
      },
      "source": [
        "## PRNG and JAX\n",
        "\n",
        "In JAX, all functions that return pseudo-random samples require you to provide the current state of the PRNG. The result includes the samples as well as the updated state of the PRNG. \n",
        "\n",
        "As the pseudo-random samples depend _solely_ on the PRNG state, passing the same state/key results in the same number being generated. This is generally undesirable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oKdk5CSmD-f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-6B0hjtlTmd"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      },
      "source": [
        "To generate different and independent samples, you need to manually **split** the keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-7BhY0MmEhI"
      },
      "outputs": [],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> new key and subkey\n",
        "new_key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the new key for future splits.\n",
        "# It doesn't really matter which key we keep and which one we use immediately.\n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> new key2 and subkey\n",
        "new_key2, subkey = random.split(new_key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnTDptmuk-i"
      },
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes, or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated). For more details on PRNG in JAX, you can read more [here](https://jax.readthedocs.io/en/latest/jep/263-prng.html). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSj972IWxTo2"
      },
      "source": [
        "## Acceleration in JAX üöÄ\n",
        "\n",
        "The key features of JAX are:\n",
        "\n",
        "* GPU/TPU accelerated operations\n",
        "* Autograd (`grad`): automatic gradient computation\n",
        "* JIT: just-in-time compilation (`jit`): create efficient accelerator-enabled code from Python code on the fly\n",
        "* Automatic vectorization (`vmap`): transparently distribute computation accross different computation cores (in the local node)\n",
        "* Automatic parallelization (`pmap`) auto. parallelization of computations accross different nodes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      },
      "source": [
        "An inportan aspect of JAX is that it is _backend agnostic_: you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code**. This means we can easily run linear algebra operations directly on GPU/TPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "**Multiplying Matrices**\n",
        "\n",
        "Let's see how JAX handles the common linear algebra operation which is the _dot product_ between a matrix and a vector:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY1RsVkXaokP"
      },
      "source": [
        "* NumPy will run on the CPU:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj59KkD_HDOs"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c_kl-u0KPVY"
      },
      "source": [
        "\n",
        "* JAX will use a GPU if available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHRcHK86KO3w"
      },
      "outputs": [],
      "source": [
        "size = 1000\n",
        "key1, key2 = jax.random.split(jax.random.PRNGKey(42), num=2)\n",
        "x = jax.random.normal(key1, shape=(size, size))\n",
        "y = jax.random.normal(key2, shape=(size, size))\n",
        "# Since JAX calls are asynchronous, we need to wait until the\n",
        "# function ends before computing the elapsed time:\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3vwh6Q724gn"
      },
      "source": [
        "**How much faster was the dot product in JAX (Using GPU)?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkASX9p34A1D"
      },
      "outputs": [],
      "source": [
        "np_average_time = np.mean(numpy_time.all_runs)\n",
        "jax_average_time = np.mean(jax_time.all_runs)\n",
        "data = {\"numpy\": np_average_time, \"jax\": jax_average_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken per framework to run dot product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**JAX not running much faster? -> Re-run the JAX cell!**\n",
        "\n",
        "An important thing to keep in mind: the first time you run JAX code, it will be slower because it is being compiled into the TPU/GPU. *This is true even if you don‚Äôt use jit in your own code, because JAX‚Äôs builtin functions are also jit compiled*.\" - [JAX Docs](https://jax.readthedocs.io/en/latest/faq.html#benchmarking-jax-code).\n",
        "\n",
        "The above taken care of, if you are running on an accelerator, you should see a considerable performance benefit of using JAX, without making any changes to your code! \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X6Rv_OQgBOqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM_08mXEBRIK"
      },
      "source": [
        "# JAX Transformations\n",
        "\n",
        "JAX transforms (`jit`, `grad`, `vmap`, `pmap`) first convert python functions into an intermediate language called *jaxpr*. Transforms are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as **tracing**. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. \n",
        "\n",
        "Any python side-effects are not recorded during tracing. JAX transforms and compilations are designed to work only with **pure functions**. For more on tracing and jaxpr, you can read [here](https://jax.readthedocs.io/en/latest/jaxpr.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsJE_U-ZzVol"
      },
      "source": [
        "## JIT (Just in Time) compilation\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we can use `jit` to compile the function the first time it is called: after its first run, all subsequent calls will use this compiled, super fast version."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIYsqIp_-Dly"
      },
      "source": [
        "Let's try JIT on the popular [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375) function:\n",
        "\n",
        "<center>$$f(x)=\\text{max}(0,x)$$</center>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFiuu3BFAKdY"
      },
      "source": [
        "**Exercise 1.2 - Code Task:** Complete the ReLU implementation below using standard python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "outputs": [],
      "source": [
        "# Implement ReLU.\n",
        "def relu(x):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zCobLakM1esy"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function.\n",
        "\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "    max_int = 5\n",
        "    # Generete 100 evenly spaced points from -max_int to max_int\n",
        "    x = np.linspace(-max_int, max_int, 1000)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    plt.plot(x, y, label=\"ReLU\")\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.xticks(np.arange(min(x), max(x) + 1, 1))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "    # Generete 100 evenly spaced points from -100 to -1\n",
        "    x = np.linspace(-100, -1, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert (y == 0).all()\n",
        "\n",
        "    # Check if x == 0\n",
        "    x = 0\n",
        "    y = relu_function(x)\n",
        "    assert y == 0\n",
        "\n",
        "    # Generete 100 evenly spaced points from 0 to 100\n",
        "    x = np.linspace(0, 100, 100)\n",
        "    y = np.array([relu_function(xi) for xi in x])\n",
        "    assert np.allclose(x, y)\n",
        "\n",
        "    print(\"Your ReLU function is correct!\")\n",
        "\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kken6_XvDdOK"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "#\n",
        "# see if it works\n",
        "#\n",
        "check_relu_function(relu)\n",
        "#\n",
        "# plot it\n",
        "#\n",
        "plot_relu(relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      },
      "source": [
        "Let's try to `jit` this function to speed up compilation and then try to call it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "outputs": [],
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "#\n",
        "# initial PRNG state\n",
        "#\n",
        "key = jax.random.PRNGKey(42)\n",
        "#\n",
        "# Generate a list of random numbers and pass them to relu\n",
        "#\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# Should raise an error.\n",
        "try:\n",
        "    relu_jit(x)\n",
        "except Exception as e:\n",
        "    print(\"Exception {}\".format(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7q33C4pHOQW"
      },
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "During JIT compilation, JAX does not encode all possible path flows of the function, but only those that were _taken_ during the call that was made when compilation happened. Fortunatelly this is detected and an error is produced at compile time.\n",
        "\n",
        "The best way to perform vector-valued operations in JAX (ad with JIT) is to use the built-in vectorized JAX operations. In this case, we can use `jax.where(a,b,c)` which will return values from `b` where `a` is True, and from `c` otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX8k4R7daBpP"
      },
      "source": [
        "**Exercise 1.3 - Code Task** : Let's convert our ReLU function above to work with jit. This can be done in several ways, e.g.,  [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) or [`jnp.maximum`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.maximum.html), if you prefer. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "outputs": [],
      "source": [
        "# Implement a jittable ReLU\n",
        "def relu(x):\n",
        "  pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fq_QRoaaG5"
      },
      "outputs": [],
      "source": [
        "# @title Run to test your function\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLtBaplGxlS3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def relu(x):\n",
        "    return jnp.where(x > 0, x, 0)\n",
        "    # Another option - return jnp.maximum(x,0)\n",
        "\n",
        "\n",
        "check_relu_function(relu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYogDOCLiLXN"
      },
      "outputs": [],
      "source": [
        "# @title Performance boosts (run me)\n",
        "\n",
        "# jit our function\n",
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "# generate random input\n",
        "key = jax.random.PRNGKey(42)\n",
        "num_random_numbers = 1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "# time normal jit function\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run for jitted function\n",
        "relu_jit(x).block_until_ready()\n",
        "\n",
        "# time jitted function\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time = np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time = np.mean(jax_jit_time.all_runs)\n",
        "data = {\"JAX (no jit)\": jax_avg_time, \"JAX (with jit)\": jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for ReLU function\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxq-z-xzs40s"
      },
      "source": [
        "## Automatic gradient computation\n",
        "\n",
        "Transformations are JAX functions that receive functions as inputs and produce functions themselves. The `grad` transformation  receives a function $f$ and produces its gradient $\\nabla{f}(x)$ as output. It can be applied to Python and NumPy functions, which means you can differentiate through loops, branches, recursion, and closures.  \n",
        "\n",
        "Let's try this on the simple scalar function $f(x)=6x^4-9x+4$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C49R8EOs-GHe"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUMepl6J-dQP"
      },
      "outputs": [],
      "source": [
        "f = lambda x: 6 * x**4 - 9 * x + 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ayvrkpiBiu4"
      },
      "source": [
        "We can compute the gradient of this function - $f'(x)$ and evaluate it at $x=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNm9hS2S-vJk"
      },
      "outputs": [],
      "source": [
        "dfdx = grad(f)\n",
        "dfdx_3 = dfdx(3.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcRUywsnF3LZ"
      },
      "source": [
        "**Exercise 1.4 - Math Task**: Can you calculate $f'(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PybYK6NEFWrD"
      },
      "outputs": [],
      "source": [
        "answer = None  # @param {type:\"integer\"}\n",
        "\n",
        "dfdx_2 = dfdx(2.0)\n",
        "\n",
        "assert (\n",
        "    answer == dfdx_2\n",
        "), \"Incorrect answer, hint https://en.wikipedia.org/wiki/Power_rule#Statement_of_the_power_rule\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex                                                  \n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\ \n",
        "f'(2) &  = 24(2)^3 -9 = 183 && \\triangleright \\textrm{Substituting x=2} \\\\\n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "CAwlhxIlRPp9",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcB5ZjojH67Q"
      },
      "source": [
        "## Higher order derivatives\n",
        "\n",
        "If we apply `grad` to the gradient/derivative of $f$ itself, $f'$, we obtain $f''$ and so on. For example, we can compute $f'''(x)$ as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "013SFq7BE54W"
      },
      "outputs": [],
      "source": [
        "d3dx = grad(grad(grad(f)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_r9VQGoIsa6"
      },
      "source": [
        "**Exercise 1.5 - Math Task**: How about $f'''(2)$ by hand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WZUArv4TInPg"
      },
      "outputs": [],
      "source": [
        "answer = None  # @param {type:\"integer\"}\n",
        "\n",
        "d3dx_2 = d3dx(2.0)\n",
        "\n",
        "assert answer == d3dx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to math task (Try not to run until you've given it a good try!') \n",
        "%%latex \n",
        "\n",
        "\\begin{aligned}\n",
        "f(x) & = 6x^4-9x+4 \\\\\n",
        "f'(x) & = 24x^3 -9  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f''(x) & = 72x^2  && \\triangleright \\textrm{Power Rule.}  \\\\\n",
        "f'''(x) & = 144x && \\triangleright \\textrm{Power Rule.} \\\\\n",
        "f'''(2) & = 144(2)=288 && \\triangleright \\textrm{Substituting x=2} \\\\ \n",
        "\\end{aligned}"
      ],
      "metadata": {
        "id": "TCC7SkH8MMVk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3QgJNU9XYyz"
      },
      "source": [
        "Another useful method is `value_and_grad`, where we can get the value ($f(x)$) and gradient ($f'(x)$). It is usually a good idea to compute both things simultaneously, as $f'$ and $f$ often involve similar operations, e.g.:\n",
        "\n",
        "$$f(x) = (\\mathbf{A}\\mathbf{x}-y)^2 \\Rightarrow \\nabla'(x) = 2(\\mathbf{A}\\mathbf{x}-y)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3zeSv6gXuyd"
      },
      "outputs": [],
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "f_x, dy_dx = value_and_grad(f)(2.0)\n",
        "print(f\"f(x): {f_x} f‚Ä≤(x): {dy_dx} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> For partial derivatives, you need to use the [`argnums`](https://jax.readthedocs.io/en/latest/_autosummary/jax.grad.html) param to specify which variables you want to differentiate with respect to. \n",
        "\n"
      ],
      "metadata": {
        "id": "_vUr-B6gSxnu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MktOLPnwvnH3"
      },
      "source": [
        "**Exercise 1.6 - Group Task:** Chat with neighbour/think about how JAX's automatic differentiation compares to other libraries such as Pytorch or Tensorflow. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful application related to `grad` is when you want your `grad` function to return auxiliary (extra) data, that you don't want differentiated. You can use the `has_aux` parameter to do this (example in \"Auxiliary data\" section in [here](https://github.com/google/jax/blob/main/docs/jax-101/01-jax-basics.ipynb))."
      ],
      "metadata": {
        "id": "rvXlE7z02M2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pure Functions\n",
        "\n",
        "JAX transformation and compilation are designed to work reliably on **pure functions** (see [Wikipedia page on pure functions](https://en.wikipedia.org/wiki/Pure_function)). These functions have the following properties:\n",
        "1. All **input** data is passed through the **function's parameters**. \n",
        "2. All **results** are output through the **function's return**. \n",
        "3. The function always returns the same **result** if invoked with the **same inputs**. What if your function involves randomness? Pass in the random seed!\n",
        "4. **No [side-effects](https://en.wikipedia.org/wiki/Side_effect_(computer_science))** - no mutation of non-local variables or input/output streams.  \n",
        " \n",
        "\n",
        "Let's see what could happen if we don't stick to using pure functions."
      ],
      "metadata": {
        "id": "fT56qxXzTVKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Side Effects"
      ],
      "metadata": {
        "id": "Mad7l7s0CtT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call print within a function."
      ],
      "metadata": {
        "id": "xkQWTE2Xe955"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def impure_print_side_effect(x):\n",
        "    print(\"Print me!\")  # This is a side-effect\n",
        "    return x\n",
        "\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print(\"First call: \", jax.jit(impure_print_side_effect)(4.0))"
      ],
      "metadata": {
        "id": "S9aeUdUoBmCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the print statement is called.\n",
        "\n",
        "Let's call this function again. "
      ],
      "metadata": {
        "id": "nu4rnyS7ox_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compilation of the function\n",
        "print(\"Second call: \", jax.jit(impure_print_side_effect)(5.0))"
      ],
      "metadata": {
        "id": "-wnkIqAxfDeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ah, no print statement! Since JAX cached the compilation of the function, `print()` calls will only happen during tracing and not every time the function is called. "
      ],
      "metadata": {
        "id": "64rNvVnwo-eB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print(\n",
        "    \"Third call, different type: \", jax.jit(impure_print_side_effect)(jnp.array([5.0]))\n",
        ")"
      ],
      "metadata": {
        "id": "Mp_CkOL-o86t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we called the function with a different shaped object and so it triggered the re-tracing of the function and print was called again. "
      ],
      "metadata": {
        "id": "XFogrIf5fbLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To print values in compiled functions, use [host callbacks](https://jax.readthedocs.io/en/latest/jax.experimental.host_callback.html?highlight=print#jax.experimental.host_callback.id_print)([example](https://github.com/google/jax/issues/196#issuecomment-1191155679)) or if your jax version>=0.3.16, you can use [`jax.debug.print`](https://jax.readthedocs.io/en/latest/debugging/print_breakpoint.html). \n"
      ],
      "metadata": {
        "id": "pqV6_25GCxHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Global variables"
      ],
      "metadata": {
        "id": "EqL1-TGaC8Ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using global variables can also lead to some undesired consequences!"
      ],
      "metadata": {
        "id": "t8dzJog8tMe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.0\n",
        "\n",
        "\n",
        "def impure_uses_globals(x):\n",
        "    return x + g\n",
        "\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print(\"First call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "vwAkKrDiCXO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This prints 4, using the original value of `g`.\n",
        "\n",
        "Let's update `g` and call our function again."
      ],
      "metadata": {
        "id": "pWNE8B5btcfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = 10.0  # Update the global\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print(\"Second call: \", jax.jit(impure_uses_globals)(4.0))"
      ],
      "metadata": {
        "id": "mLMpdQZwtUEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though we updated our global variable, this still prints 4, using the original value of `g`. This is because the value of `g` was cached."
      ],
      "metadata": {
        "id": "o3-ygEx0tpBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print(\"Third call, different type: \", jax.jit(impure_uses_globals)(jnp.array([4.0])))"
      ],
      "metadata": {
        "id": "LDecWNyktWDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the side-effects example, re-tracing gets triggered when the shape of our input has changed. In this case, our function now uses the updated value of `g`."
      ],
      "metadata": {
        "id": "3mIZaXOqt5ix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the global variables are cached, it is still okay to use global **constants** inside jax functions."
      ],
      "metadata": {
        "id": "aLis2BV04BQK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUB9YkCnCFb"
      },
      "source": [
        "## Function vectorization with vmap\n",
        "\n",
        "The vmap transform allows you to easily distribute computation of identical operations accross different units within an acceleration unit. We'll try this on  a simple function that calculates the min and max of an input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6qalyXgDsKB"
      },
      "outputs": [],
      "source": [
        "def min_max(x):\n",
        "    return jnp.array([jnp.min(x), jnp.max(x)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSIsUkgKlxh"
      },
      "source": [
        "We can apply this function to the vector - `[0, 1, 2, 3, 4]` and get the min and max values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5wIeGieKsWG"
      },
      "outputs": [],
      "source": [
        "x = jnp.arange(5)\n",
        "min_max(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PkC7NnPLNXq"
      },
      "source": [
        "What about if we want to apply this to a batch/list of vectors (i.e. calculate the min and max independently across multiple batches)? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRngFfwCMHLd"
      },
      "source": [
        "Let's create our batch - 3 vectors of size 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKuh459OD6jx"
      },
      "outputs": [],
      "source": [
        "batch_size = 3\n",
        "batched_x = np.arange(15).reshape((batch_size, -1))\n",
        "print(batched_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApYpVEvNS1y"
      },
      "source": [
        "**Exercise 1.7 - Question**: What do you think would be the result if we passed batch_x into `min_max`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu6C3J0kMrtj"
      },
      "outputs": [],
      "source": [
        "batch_min_max_output = [[0,4],[5,9],[10,14]]  # @param [\"[[0,4],[5,9],[10,14]]\", \"[[0,10],[1,11],[2,12],[3,13],[4,14]]\", \"[0,14]\"] {type:\"raw\"}\n",
        "\n",
        "assert (batch_min_max_output == np.array(min_max(batched_x))).all(), \"Incorrect answer.\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0weiHOOb8L"
      },
      "source": [
        "So the above is not what we want. The `min` and `max` is applied across the entire batch, when we want the min and max per vector/mini-batch. \n",
        "\n",
        "We can also manually batch this by `jnp.stack` and a for loop, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8RdAqr8N-Fd"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "    min_max_result_list = []\n",
        "    for x in batched_x:\n",
        "        min_max_result_list.append(min_max(x))\n",
        "    return jnp.stack(min_max_result_list)\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmu3VVtMR0GV"
      },
      "source": [
        "Or, just manually updating the `axis` in `jnp.min` and `jnp.max`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzxmORv-RcUg"
      },
      "outputs": [],
      "source": [
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "    return jnp.array([jnp.min(batched_x, axis=1), jnp.max(batched_x, axis=1)]).T\n",
        "\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CetKYASUSE4Q"
      },
      "source": [
        "These approaches both work, but we need to change our function to work with batches. We can't just run the same code across a batch of data.\n",
        "\n",
        "There is where `vmap` becomes useful! Using `vmap` we can write a function once, as if it is working on a single element, and then use `vmap` to automatically vectorize it! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2F8WUNQROkQ"
      },
      "outputs": [],
      "source": [
        "# define our vmap function using our original single vector function\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "    return vmap(min_max)(batched_x)\n",
        "\n",
        "\n",
        "# Run it on a single vecor\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_leading_dim = jax.numpy.expand_dims(x, axis=0)\n",
        "print(f\"Single vector: {min_max_vmap(x_with_leading_dim)}\")\n",
        "\n",
        "# Run it on batch of vectors\n",
        "print(f\"Batch/list of vector:{min_max_vmap(batched_x)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bome92VRL6"
      },
      "source": [
        "So this is really convenient, but what about performance? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1Nb4uniUUor"
      },
      "outputs": [],
      "source": [
        "batched_x = np.arange(50000).reshape((500, 100))\n",
        "\n",
        "# Trace the functions with first call\n",
        "manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "min_max_forloop_time = %timeit -o -n 10 manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "min_max_axis_time = %timeit -o -n 10 manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap_time = %timeit -o -n 10 min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "print(\n",
        "    f\"Avg Times (lower is better) - Naive Implementation: {np.round(np.mean(min_max_forloop_time.all_runs),5)} Manually Vectorized: {np.round(np.mean(min_max_axis_time.all_runs),5)} Vmapped Function: {np.round(np.mean(min_max_vmap_time.all_runs),5)} \"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYL758zCYsrR"
      },
      "source": [
        "So `vmap` should be similar in performance to manually vectorized code (if everything is implemented well), and much better than naively vectorized code (i.e. for loops). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAO9dOdrtiqI"
      },
      "source": [
        "## Paralelization with pmap (Optional)\n",
        "\n",
        "üí°**For this subsection, please ensure that colab is using a `TPU` runtime. If no `TPU` runtimes are available, select `Harware Accelerator` - `None` for a cpu runtime.** \n",
        "\n",
        "With `pmap` we can convert a function written for a single device to a function that can run in parallel across many devices. \n",
        "\n",
        "**Difference between `vmap` and `pmap`**:\n",
        "\n",
        "So both `pmap` and `vmap` transform a function to work over an array, but they differ in implementation. `vmap` adds an extra batch dimension to all the operations in a function, while `pmap` replicates the function and executes each replica on its own XLA device in parallel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUYA277soR-0"
      },
      "outputs": [],
      "source": [
        "# @title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qhlBnLs6AYL"
      },
      "source": [
        "Let's try and `pmap` a batch of dot products.\n",
        "\n",
        "Here is an illustration of how we would typically do this sequentially: \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz1i2AwA5_7J"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Sequential Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/not_parallel-2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTmWNFZ08f8n"
      },
      "source": [
        "Here is the code implementation of this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTuMldJ9Uv5"
      },
      "outputs": [],
      "source": [
        "# Let's generate a batch of size 8, each with a matrix of size (500, 600)\n",
        "\n",
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Let create our batches\n",
        "mats = jnp.stack([jax.random.normal(key, (500, 600)) for key in keys])\n",
        "\n",
        "\n",
        "def dot_product_sequential():\n",
        "    @jit\n",
        "    def avg_dot_prod(mats):\n",
        "        result = []\n",
        "        # Loop through batch and compute dp\n",
        "        for mat in mats:\n",
        "            # dot product between the a mat and mat.T (transposed version)\n",
        "            result.append(jnp.dot(mat, mat.T))\n",
        "        return jnp.stack(result)\n",
        "\n",
        "    avg_dot_prod(mats).block_until_ready()\n",
        "\n",
        "\n",
        "run_sequential = %timeit -o -n 5 dot_product_sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEtecJX-0AW"
      },
      "source": [
        "Here is an illustration of how we would do this in parallel \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uswxurmn-5oC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Illustration of Parallel Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    '<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/parallelized.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGsq8iTA_N9U"
      },
      "source": [
        "Here is code implementation of batched dot products:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ygFWDfQIoeC"
      },
      "source": [
        "First, we will create `8` random matrices (one for each available tpu devices - colab tpu's have 8 available [devices](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) or the 8 cpu cores as we configured)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZLMx06_K_qR"
      },
      "outputs": [],
      "source": [
        "# Let create 8 keys, 1 for each batch\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "\n",
        "# Each replicated pmapped function get a different key\n",
        "mats = pmap(lambda key: jax.random.normal(key, (500, 600)))(keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkMsaOtLISj"
      },
      "source": [
        "The leading dimension here needs to equal the dimension of available devices (since we are sending a batch to each device)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWrdv_2wLG4T"
      },
      "outputs": [],
      "source": [
        "print(mats.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnqblcUsLaKZ"
      },
      "source": [
        "Using `pmap` to generate the batches ensures these batches are of type `ShardedDeviceArray`. This is similar to an ndarray, except each batch/shared is stored in the memory of multiple devices, so they can be used in subsequent `pmap` operations without moving data around between devices (GPU/TPU) and hosts (cpu). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAeaBCvcLQWg"
      },
      "outputs": [],
      "source": [
        "print(type(mats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVz0gOWG9pkr"
      },
      "outputs": [],
      "source": [
        "def dot_product_parallel():\n",
        "\n",
        "    # Run a local matmul on each device in parallel (no data transfer)\n",
        "    result = pmap(lambda x: jnp.dot(x, x.T))(\n",
        "        mats\n",
        "    ).block_until_ready()  # result.shape is (8, 5000, 5000)\n",
        "\n",
        "\n",
        "run_parallel = %timeit -o -n  5 dot_product_parallel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64gfyF3ENQzU"
      },
      "source": [
        "It is simple as that. Our dot product now runs in parallel across available devices (cpu, gpus or tpus). As we have more cores/devices, this code will automatically scale! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qcQXSbANP_M",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Let's plot the performance difference (Run Cell)\n",
        "\n",
        "jax_parallel_time = np.mean(run_parallel.all_runs)\n",
        "jax_seq_time = np.mean(run_sequential.all_runs)\n",
        "\n",
        "\n",
        "data = {\"JAX (seq)\": jax_seq_time, \"JAX (parallel - pmap)\": jax_parallel_time}\n",
        "\n",
        "plot_performance(data, title=\"Average time taken for Seq vs Parallel Dot Product\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0j8iJRFUz6v"
      },
      "source": [
        "For some problems, the speed can be directly proportional to the number of devices -- $Nx$ speed up for $N$ devices! \n",
        "\n",
        "We showed an example of using `pmap` for *pure* parallelism, where there is no communication between devices. JAX also has various operations for communication across distributed devices ( more on this [here](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#communication-between-devices).)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAtms17jtCOU"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 2 - Optimization using Optax\n",
        "\n",
        "<hr>\n",
        "\n",
        "[Optax](https://github.com/deepmind/optax), leverages from the capabilities of JAX, in particular automatic differentiation, to run optimization methods such as vanilla gradient descent, or more advanced ones such as ADAM, on any loss function we may have. \n",
        "\n",
        "Below we show how to use Optax to train a simple linear model of the form \n",
        "\n",
        "$$y=w\\tr x + b,\\,w \\in \\reals^m,\\;y,b \\in \\reals.$$\n",
        "\n",
        "Below is a little function to simulate noisy samples drawn from a linear model, and a classic mean squared loss function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import random\n",
        "\n",
        "def sim_linear_model(w, b, num_samples, noise_std=0.05):\n",
        "  \"\"\"\n",
        "  given a linear coefficientw vector w, a bias b and a non-negative integer n\n",
        "  generate n samples according to y = wx + b + v\n",
        "  where v is a random Gaussian variable with mean 0\n",
        "  and standard deviation 0.1\n",
        "  \"\"\"\n",
        "  sample_dim = len(w)\n",
        "  X = random.uniform(size=(sample_dim,num_samples))\n",
        "  noise = noise_std*random.normal(size=num_samples)\n",
        "  y = w @ X + b + noise\n",
        "  return X,y\n",
        "\n",
        "\n",
        "def loss(X, y, w, b):\n",
        "  \"\"\"\n",
        "  JAX-based implementation of mean squared loss\n",
        "  \"\"\"\n",
        "  errors = jnp.square(y - w.T @ X - b)\n",
        "  return jnp.mean(errors)\n",
        "\n",
        "#\n",
        "# ground truth\n",
        "#\n",
        "w = [0.4]\n",
        "b = -0.2\n",
        "#\n",
        "# draw samples\n",
        "#\n",
        "X,y = sim_linear_model(w, b, 100)\n"
      ],
      "metadata": {
        "id": "AKuLize6bkzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optax workflow\n",
        "\n",
        "Since JAX is pure function-based, Optax functions need to be pure functions too. Thus, in order to update the model's parameters, two functions are called in sequence:\n",
        "\n",
        "1. `delta_theta, new_opt_state = optimizer.update(theta,opt_state)` given the current value of the model parameters, $\\theta$, and the current internal state of the optimizer, this function returns the update $\\Delta\\theta$ that needs to be applied to theta in order to obtain a new iterate, and the new state of the optimizer. The latter contains past information gathered by the optimizer which might influence the computation of $\\Delta \\theta$.\n",
        "1. `optax.apply(theta,delta_theta)` simply adds `delta_theta` to `theta`\n",
        "\n",
        "One important aspect of Optax is that it expects the model parameters to be provided as name-value pairs in a dictionary. In the linear model below we have two parameters $w$ and $b$ so that $\\theta=(w,b)$ and $\\Delta\\theta=(\\Delta w,\\Delta b)$.\n"
      ],
      "metadata": {
        "id": "cY6EQK582Phk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U optax\n",
        "\n",
        "import optax\n",
        "import jax.numpy.linalg as jla\n",
        "\n",
        "# \n",
        "# JAX autograd:  the returned value gloss is a function \n",
        "# that computes the gradient of loss(X,y,w,b) with respect to w and b (args 2 and 3)\n",
        "# \n",
        "gloss = jax.grad(loss, argnums=(2,3))\n",
        "\n",
        "\n",
        "def train_model_optax(X, y, fun, grad, stepsize=1e-2,tolerance=1e-4,maxiter=1000):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param X: reference inputs\n",
        "  :param y: reference output\n",
        "  :param fun: cost function \n",
        "  :param grad: gradient of the cost function\n",
        "  :param stepsize: step size to apply\n",
        "  :param tolerance: stop if improvement is less than this value\n",
        "  :param maxiter: stop if we reach this number of iterations\n",
        "  \"\"\"\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  #\n",
        "  optimizer = optax.sgd(stepsize) # TRY other optimizers and stepsizes!! optax.adam\n",
        "  #\n",
        "  # initial parameters\n",
        "  #\n",
        "  # Optax expects a dictionary parameters\n",
        "  # in this case we have only one: w\n",
        "  #\n",
        "  theta_t = {\"w\": jnp.zeros(X.shape[0]), \"b\":jnp.ones(1)}\n",
        "  #\n",
        "  # all Optax optimizers have an inner state that needs to \n",
        "  # be stored and updated between calls\n",
        "  #\n",
        "  state_t = optimizer.init(theta_t)\n",
        "  \n",
        "  params     = list()\n",
        "  costs      = list()\n",
        "  grad_norms = list()\n",
        "  #\n",
        "  # main optimization loop\n",
        "  #\n",
        "  for t in range(maxiter):\n",
        "    #\n",
        "    # values at current iteration\n",
        "    #\n",
        "    w_t = theta_t[\"w\"]\n",
        "    b_t = theta_t[\"b\"]\n",
        "    g_t = grad(X,y,w_t,b_t)\n",
        "    f_t = fun(X,y,w_t,b_t)\n",
        "    n_t = jla.norm(g_t)\n",
        "    #\n",
        "    # check for stopping condition\n",
        "    #\n",
        "    if n_t < tolerance:\n",
        "      break\n",
        "    #\n",
        "    # store current solution\n",
        "    #\n",
        "    params.append(theta_t)\n",
        "    costs.append(f_t)\n",
        "    grad_norms.append(n_t)\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "    # Optax expects a dictionary of gradients, one per parameter\n",
        "    # corresponding to the dictionary of parameters above\n",
        "    #\n",
        "    g_t = {\"w\":g_t[0],\"b\":g_t[1]} \n",
        "    d_t, state_t = optimizer.update(g_t, state_t)\n",
        "    theta_t = optax.apply_updates(theta_t, d_t)\n",
        "\n",
        "  return params,costs,grad_norms\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "a1u1ZpbAqRTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running our optax-powered optimizer\n",
        "\n",
        "Run the cell below to see our latest method in action.\n",
        "\n",
        "<hr>\n",
        "\n",
        "**Excercise:** try again, now changing the optimizer and/or the stepsize (see [Common Optimizers](https://optax.readthedocs.io/en/latest/api.html) ). One popular one is ADAM (`optax.adam`)\n"
      ],
      "metadata": {
        "id": "j4Sxc76A49jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trajectory(params,costs,norms):\n",
        "  \"\"\"\n",
        "  helper function for plotting our thing\n",
        "  \"\"\"\n",
        "  niter = len(costs)\n",
        "\n",
        "  plt.figure(figsize=(15,5))\n",
        "\n",
        "  plt.subplot(1,3,1)\n",
        "  plt.semilogy(costs)\n",
        "  plt.xlabel('iteration')\n",
        "  plt.ylabel('loss')\n",
        "  plt.title('evolution of loss across iterations')\n",
        "  plt.grid(True)\n",
        "\n",
        "#\n",
        "# call the algorithm\n",
        "#\n",
        "params,costs,norms = train_model_optax(X, y, loss, gloss,stepsize=0.1)\n",
        "\n",
        "plot_trajectory(params,costs,norms)"
      ],
      "metadata": {
        "id": "YBN1Ov1dta8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exuVety_bFhQ"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 3 - Training Models with Haiku\n",
        "\n",
        "<hr>\n",
        "\n",
        "So far we've seen how to use JAX and Optax to implement an optimization loop.\n",
        "Thanks to these two libraries, we can exploit our computing resources to the maximum, forget about computing gradients by hand, and to define the descent direction altogether.\n",
        "\n",
        "What if we now want to build a deep neural model? If we want to do it with JAX, we need JAX-enabled blocks (dense, convolutional, relus, maxpool, etc.) and a way to construct and maintain the state of the resulting mammoth.\n",
        "\n",
        "Here's where [Haiku](https://github.com/deepmind/dm-haiku) comes in, by providing a JAX-based alternative to those familiar with  the traditional Pytorch or Tensorflow libraries.\n",
        "\n",
        "## Haiku is object oriented\n",
        "\n",
        "Haiku is an *object-oriented* library, meaning that our models, their parameters, etc., are represented as _instances_ of _object classes_. If you are not familiar with object oriented programming, you should learn this concept first. There are many tutorials on the subject ([example](https://realpython.com/python3-object-oriented-programming/)).\n",
        "\n",
        "## JAX is functional oriented\n",
        "\n",
        "However, there is a twist, as JAX follows the  _functional programming_ paradigm! \n",
        "\n",
        "Haiku modules are similar to standard python objects (they have references to their own parameters and functions). However, since JAX operates on *pure functions*, Haiku modules **cannot be directly instantiated**, but rather they need to be **wrapped into pure function transformations.**\n",
        "\n",
        "So, the overall rationale is to:\n",
        "\n",
        "* write an object for your model\n",
        "* wrap the object so that it is seen by JAX as a set of pure functions\n",
        "\n",
        "Don't worry. It sounds more complicated than it is."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "#@title Run this cell to install Haiku!\n",
        "!pip install -U dm-haiku\n"
      ],
      "metadata": {
        "id": "lMubruAF4ZDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wvTzTi-YJTp"
      },
      "source": [
        "## Wrapping a model in Haiku\n",
        "\n",
        "We'll now create a linear model in the way of Haiku. This linear model contains another parameter `b`, a constant (bias) term that is added to the linear combination, so that $y=wx+b$.\n",
        "\n",
        "The core object in Haiku is a `Module`. A module contains parameters and a function `__call__` that combines these parameters and user input to produce an output. This is what we could call a `block` within a Neural Network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_-3r49B-Orc"
      },
      "outputs": [],
      "source": [
        "import haiku as hk\n",
        "\n",
        "class MyLinearModel(hk.Module): # notice: model inherits from haiku.Module\n",
        "  \"\"\"\n",
        "  Haiku-based linear model of the form y=w*x+b\n",
        "\n",
        "  A Haiku module is expected to implement __call__ in order to produce\n",
        "  its output.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_dim, name=None):\n",
        "    \"\"\"\n",
        "    :param output_dim: dimension of the model output (y)\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    special function, gets called when we use the () operator on an instance\n",
        "    \"\"\"\n",
        "    j, k = x.shape[-1], self.output_dim\n",
        "    #\n",
        "    # note: the three lines below are actually called *once* and serve\n",
        "    # to declare and initialize the parameters of the model.\n",
        "    # \n",
        "    w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "    #\n",
        "    # this is the actual code that's executed when the module is called\n",
        "    #\n",
        "    return jnp.dot(x, w) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XGOeJCH-10P"
      },
      "source": [
        "### Wrapping the Module\n",
        "\n",
        "In order for a module to be used by JAX, it needs to be treated not as an object, but as a set of pure functions plus a _state_, which has to be explicitly fed to the functions.\n",
        "\n",
        "The wrapping is done by the Haiku `transform` function. This wrapper takes a **function** of one or more Haiku Modules (not just any class) and \"eviscerates\" the modules it to bring out the parameters. \n",
        "\n",
        "In order for Haiku to know which parameters each  module has, the parameters need to be explicitly created and initialized using `hk.get_parameter`. In the module above, this is done in the `__call__` functions. \n",
        "\n",
        "(Admittedly, this is rather counter-intuitive, as it would seem that the parameters are created and initialized each time the `__call__` function is called. Also, they are not in the `__init__` method, which is the natural place for initializing things in an object. )\n",
        "\n",
        "So, the wrapping goes in two stages:\n",
        "\n",
        "1. we wrap the model (module) into a function\n",
        "1. we transform the function to obtain two other functions:\n",
        "   * an `init` function, that serves to initialize the parameters\n",
        "   * an `apply` function, which calls the function with the parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1yI7j2h_Esd"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# first step: wrap the module in a function\n",
        "#\n",
        "def model_fn(x):\n",
        "  \"\"\"\n",
        "  wrapper function for our model\n",
        "  \"\"\"\n",
        "  module = MyLinearModel(output_dim=1)\n",
        "  return module(x)\n",
        "\n",
        "#\n",
        "# second step: transform the function into a pair (init,apply)\n",
        "# \n",
        "wrapped_model = hk.transform(model_fn)\n",
        "print(wrapped_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Different flavours of transformation\n",
        "\n",
        "As you may know from our previous tutorial on JAX, pseudo-random numbers play a central role in JAX and, because of the pure function concept, their state needs to be passed explicitly anywhere they're used.\n",
        "\n",
        "With this in mind, the functions produced by `hk.transform` include the PRNG state explicitly as their first argument in all calls, as can be seen in the  signature of the wrapper functions returned by `hk.transform`:\n",
        "\n",
        "* `init(prng,data)`\n",
        "* `apply(prng,params,data)`\n",
        "\n",
        "If we don't want or don't need to use a PRNG, we can ask for a different set of wrapper functions which do not include the PRNG as an argument. In our case, our `init` function does use PRNG since the weights are initialized randomly through `hk.initializers.TruncatedNormal`, but  our module's output is deterministic ($y=wx+b$). Accordingly, we'd like our signatures to be:\n",
        "\n",
        "* `init(prng)`\n",
        "* `apply(params,input)`\n",
        "\n",
        "This is achieved by calling `hk.without_apply_rng` to our transformed model:"
      ],
      "metadata": {
        "id": "i0OC1iQVS71H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# transform again to obtain another pair \n",
        "#\n",
        "wrapped_model = hk.without_apply_rng(wrapped_model)\n",
        "#\n",
        "# inspect the output and notice how the `apply` function has changed \n",
        "# compared to the previous value\n",
        "#\n",
        "print(wrapped_model)"
      ],
      "metadata": {
        "id": "Bv-q5rw7S7d_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lao8wS3tBjc3"
      },
      "source": [
        "### Test the wrapper model\n",
        "\n",
        "Let's see how the wrapped model works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt0srU3rQlhL"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# input dimention we are considering\n",
        "#\n",
        "input_dim = 3\n",
        "#\n",
        "# evaluate our model somewhere\n",
        "#\n",
        "some_x= jnp.arange(input_dim, dtype=jnp.float32)\n",
        "print(some_x)\n",
        "#\n",
        "# initialize PRNG, a must in JAX\n",
        "#\n",
        "rng_key = jax.random.PRNGKey(42)\n",
        "#\n",
        "# initialization DOES use the PRNG, and (possibly) the data too\n",
        "#\n",
        "params = wrapped_model.init(rng=rng_key, x=some_x)\n",
        "print(params)\n",
        "#\n",
        "# try it out\n",
        "#\n",
        "print(wrapped_model.apply(params,some_x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IaqVuRPg3ER"
      },
      "source": [
        "# Training with Haiku and Optax\n",
        "\n",
        "\n",
        "Here we show a full training loop, using Haiku and Optax. For convenience, we introduce structures like `TrainingState` and functions like `init`,`update` and `loss_fn`. Please read through to get comfortable with how you can effectively train JAX models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load packages\n",
        "%%capture\n",
        "# need to install this for plotting.\n",
        "!pip install optax\n",
        "!pip install dm-haiku\n",
        "#\n",
        "# packages\n",
        "#\n",
        "from typing import Any, MutableMapping, NamedTuple, Tuple\n",
        "import time\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "import haiku as hk\n",
        "import optax\n"
      ],
      "metadata": {
        "id": "LY0t6C4OKzSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define and wrap our model\n",
        "\n",
        "class MyLinearModel(hk.Module): # notice: model inherits from haiku.Module\n",
        "  \"\"\"\n",
        "  Haiku-based linear model of the form y=w*x+b\n",
        "\n",
        "  A Haiku module is expected to implement __call__ in order to produce\n",
        "  its output.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, output_dim, name=None):\n",
        "    \"\"\"\n",
        "    :param output_dim: dimension of the model output (y)\n",
        "    \"\"\"\n",
        "    super().__init__(name=name)\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "\n",
        "  def __call__(self, x):\n",
        "    \"\"\"\n",
        "    special function, gets called when we use the () operator on an instance\n",
        "    \"\"\"\n",
        "    j, k = x.shape[-1], self.output_dim\n",
        "    #\n",
        "    # note: the three lines below are actually called *once* and serve\n",
        "    # to declare and initialize the parameters of the model.\n",
        "    # \n",
        "    w_init = hk.initializers.TruncatedNormal(1.0 / np.sqrt(j))\n",
        "    w = hk.get_parameter(\"w\", shape=[j, k], dtype=x.dtype, init=w_init)\n",
        "    b = hk.get_parameter(\"b\", shape=[k], dtype=x.dtype, init=jnp.ones)\n",
        "    #\n",
        "    # this is the actual code that's executed when the module is called\n",
        "    #\n",
        "    return jnp.dot(x, w) + b\n",
        "\n",
        "#\n",
        "# wrap the model into a function\n",
        "#\n",
        "def model_fn(x):\n",
        "  \"\"\"\n",
        "  same wrapper function for Haiku modules, repeated for clarity\n",
        "  \"\"\"\n",
        "  module = MyLinearModel(output_dim=1)\n",
        "  return module(x).ravel()\n",
        "#\n",
        "# transform using Haiku as a pair init,apply\n",
        "#\n",
        "model = hk.without_apply_rng(hk.transform(model_fn))\n"
      ],
      "metadata": {
        "id": "0epd5-xgZPcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random as jrand\n",
        "from jax.numpy import linalg as jla\n",
        "\n",
        "def loss_fun_haiku(params: hk.Params, X, y):\n",
        "  \"\"\"\n",
        "  our loss function\n",
        "  \"\"\"\n",
        "  y_pred = model.apply(params, X)\n",
        "  return jnp.mean((y_pred - y) ** 2)\n",
        "\n",
        "\n",
        "def train_model_haiku(X, y, model, loss_fun, stepsize=1e-3,tolerance=1e-4,maxiter=1000,seed=42):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param X: reference inputs\n",
        "  :param y: reference output\n",
        "  :param model: model to be optimized\n",
        "  :param loss_fun: cost function \n",
        "  :param stepsize: gradient of the cost function\n",
        "  :param tolerance: stop if improvement is less than this value\n",
        "  :param maxiter: stop if we reach this number of iterations\n",
        "  :param seed: random seed\n",
        "  \"\"\"\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  #\n",
        "  optimizer = optax.sgd(stepsize) # TRY other optimizers and stepsizes!! optax.adam\n",
        "  #\n",
        "  # initialization\n",
        "  #\n",
        "  rng = jrand.PRNGKey(seed)\n",
        "  rng, init_rng = jrand.split(rng)\n",
        "  theta_t       = model.init(init_rng, X)\n",
        "  state_t       = optimizer.init(theta_t)\n",
        "  \n",
        "  params     = list()\n",
        "  losses     = list()\n",
        "  grad_norms = list()\n",
        "  #\n",
        "  # main optimization loop\n",
        "  #\n",
        "  for t in range(maxiter):\n",
        "    #\n",
        "    # values at current iteration\n",
        "    #\n",
        "    loss_t, gtheta_t  = jax.value_and_grad(loss_fun)(theta_t, X, y)\n",
        "    n_t = jnp.sqrt(jnp.sum(jnp.square(gtheta_t[\"my_linear_model\"][\"w\"])) + jnp.sum(jnp.square(gtheta_t[\"my_linear_model\"][\"b\"])))\n",
        "    #\n",
        "    # check for stopping condition\n",
        "    #\n",
        "    if t > 0 and t % 100: \n",
        "      print(n_t)\n",
        "      \n",
        "    if n_t < tolerance:\n",
        "      break\n",
        "    #\n",
        "    # store current solution\n",
        "    #\n",
        "    params.append(theta_t)\n",
        "    losses.append(loss_t)\n",
        "    grad_norms.append(n_t)\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "    dtheta_t, state_t = optimizer.update(gtheta_t, state_t)\n",
        "    theta_t           = optax.apply_updates(theta_t, dtheta_t)\n",
        "\n",
        "  return params,losses,grad_norms\n",
        "\n",
        "\n",
        "params,losses,norms = train_model_haiku(X, y, model, loss_fun_haiku, stepsize=1e-2)\n",
        "\n",
        "plot_trajectory(params,losses,norms)\n",
        "    \n"
      ],
      "metadata": {
        "id": "29k-YKgkaspy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# PART 4 - Convolutional Neural Networks\n",
        "\n",
        "<hr>\n",
        "\n",
        "We have seen how to create and train a simple model using Haiku. Now we will use the built in models and blocks of Haiku to create and train a deep convolutional network. In the process, we will learn about convolutional networks, one of the core blocks in modern neural models.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2-ESLy53gDLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Module Prerequisites (run if you skipped previous modules)\n",
        "!pip install optax # repeated install, if you want to run just PART 3\n",
        "!pip install dm-haiku \n",
        "\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import os # for file i/o\n",
        "\n",
        "#\n",
        "# basic libraries\n",
        "#\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#\n",
        "# JAX, Optax and Haiku\n",
        "#\n",
        "import jax\n",
        "from jax import numpy as jnp\n",
        "from jax import random as jrand\n",
        "from jax.numpy import linalg as jla\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import optax\n",
        "\n",
        "import haiku as hk\n",
        "\n",
        "import tensorflow as tf # for the datasets\n",
        "\n",
        "\n",
        "if \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "\n"
      ],
      "metadata": {
        "id": "ywc4u5sm4XVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5_zoqk-YK0D"
      },
      "source": [
        "## Preliminaries - color image representation\n",
        "\n",
        "As you may know, there are different types of digital images. The simplest ones are grayscale images, where a pixel is a single scalar value (e.g., 254) where the value 0 is black and the value 255 is white (if pixels are floating point values, the convention is that 0.0 is black and 1.0 is white). \n",
        "\n",
        "In general, however, an image can have several _channels_. Typical color images have three channels: red, green and blue (RGB - generally in that order). In such cases, an RGB pixel's value is a _vector_ of three values `[r,g,b]`.\n",
        "\n",
        "More generally, an _hyperspectral image_ (e.g., with infrared bands) can have any number of channels.\n",
        "\n",
        "The convention used in Python image processing/computer vision/machine learning libraries is to represent images as _three-dimensional tensors_. There are two conventions:  _channels first_ and _channels last_. As the name implies, a _channels first_ image is a $c{\\times}h{\\times}w$ array where $h$ and $w$ are the height and width of the image, and $c$ is the number of channels. So for example, a typical 640x480 RGB image will have dimensions 3x640x480.\n",
        "\n",
        "The more common convention is the _channels last_ one. In this case, a 640x480 image will have dimension 640x480x3, where the _last_ index indicates the channel.\n",
        "\n",
        "\n",
        "## Color images as network inputs\n",
        "\n",
        "As input data, even small images are huge. To see this, consider a simple, shallow network with a single fully connected layer of 500 neurons whose inputs are 100x100 RGB images: **how many parameters would this network have?**. Do this calculation  before continuing!\n",
        "\n",
        "## Convolutional layers\n",
        "\n",
        "A fully connected layer assigns a unique set of weights to each input. As we've seen, this quickly becomes prohibitive when the number of inputs is very large. \n",
        "\n",
        "Fortunatelly, when dealing with images, we can leverage the concept of _invariance_, more specifically _translation invariance_, to reduce the number of parameters. The idea is that relevant image features do not depend on where they are, e.g., we may not care where a face appears in the image. \n",
        "\n",
        "\n",
        "Convolutional architectures exploit this by reusing the same weights all over the image: for each input pixel, we consider a small _window_, _region_ or _neighborhood_ around it. An output is then produced by weighting the values of this set of pixels depending on the relative position of the pixel within the region. For an input with $c$ channels, we simple assign a set of $c$ different weights to each relative position. The set of values associated to a window is called a _filter_ or _kernel_. \n",
        "### Feature maps\n",
        "\n",
        "Notice that the output of the layer now depends on the _position_ of the region. In the end, we obtain a value for each position of the input image: this may be considered as a kind of _pseudo-image_ or _map_ which is often called activation map. \n",
        "\n",
        "### Filters\n",
        "\n",
        "Typical regions used in convolutional layers are about $3{\\times}3$ or $5{\\times}5$ pixels. This size is called _kernel size_, because it also defines the number of weights in the filter.\n",
        "\n",
        "Of course, having just one filter for the whole image (say, $25$ weights) is far too low to capture any relevant information. Instead, convolutional layers consist of several (e.g., $k=100$ or $k=200$) such filters. If we consider all the $k$ output maps together, we obtain a multi-channel output pseudo-image where each channel corresponds to the output of a given filter.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JbXSVKulDoD"
      },
      "source": [
        "\n",
        "The following animation illustrates these ideas:\n",
        "\n",
        "![Convolution Animation](https://i.stack.imgur.com/FjvuN.gif)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLiuT6TcmXw8"
      },
      "source": [
        "Based on the above introduction, we have the two main parameters  of a convolutional layer:\n",
        "\n",
        "* **number of filters** defines the number of filters in the layer\n",
        "* **kernel size** defines the width and height of the neighborhoods the filters (also called \"kernels\") in the layer. Note that kernels always have the same depth as the inputs to the layer.\n",
        "\n",
        "Besides these, other important ones are:\n",
        "\n",
        "* **Stride** defines the number of pixels by which we move the filter when \"sliding\" it along the input volume. Typically this value would be 1, but values of 2 and 3 are also sometimes used, in which case the output map size is reduced by this factor (has a lower resolution).\n",
        "* **Padding** when computing the values of the border pixels, we need to fill in the values of the neighbors that fall outside of the image. Typically we have three strategies: _valid_, which refuses to compute values at the borders; _same_, which repeats the border pixel values to the outside and _zero_, which just assumes them to be zero.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW5FkOM2EOr3"
      },
      "source": [
        "## Stacking convolutional layers for more complex representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEtb0aaeZ6os"
      },
      "source": [
        "ConvNet architectures were key to the tremendous success of deep learning in machine vision. In particular, the first deep learning model to win the ImageNet competition in 2012 was called AlexNet (after Alex Krizhevsky, one of its inventors). It had 5 convolutional layers followed by 3 fully-connected layers. Later winners included GoogLeNet and ResNet. If you're curious, have a look at [this link](https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba) for a great summary of different ConvNet architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F__JkeqMEbzv"
      },
      "source": [
        "The ability to build complex filters by stacking several convolutional layers is key in the success of these types of networks. For example, imagine a 5 layer CNN that has been trained to detect faces. The first 4 layers are convolutional and the last layer is fully-connected and outputs the final prediction (is there a face or not). Experience shows that the filters in upper layers are able to capture higher level features. For example, in the example  mentioned above, learned filters typically respond to (in order of layer):\n",
        "\n",
        "1. lines (horizontal, vertical, diagonal), and colour gradients,\n",
        "2. corners, circles and other simple shapes, and simple textures,\n",
        "3. noses, mouths, and eyes,\n",
        "4. whole faces.\n",
        "\n",
        "This can be seen in the following images taken from [this paper](http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf))\n",
        "\n",
        "![Imgur](https://i.imgur.com/653uIty.jpg)\n",
        "\n",
        "Check out [this paper](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf) for more examples.\n",
        "\n",
        "## Receptive fields\n",
        "\n",
        "As you can see above, not only the features are more complex, but they also span a larger region, far greater than $5{\\times}5$. This is because the region a kernel sees in layer $j$ depends on various regions on the layer $j-1$. So, as we go up in the layers, the effective region or _receptive field_ that a neuron sees becomes larger and larger (a term dating back to the origin of the Perceptron in the 1950's). \n",
        "\n",
        "## Pooling\n",
        "\n",
        "Convolutional networks often (almost always) interleave convolutional layers with so called _pooling layers_. These are non-trainable layers that reduce the size of activation maps by producing a single output per region. This may be done for two reasons:\n",
        "\n",
        "* To combine the outputs of various filters into one. For example, we may be interested in keeping the output of the filter with _maximum_ value. This is known as a _max pool_.\n",
        "* To combine elements of the activation map in the _spatial_ domain, that is, nearby outputs into one. This may be done by averaging, taking the maximum, etc., of regions. Contrary to convolutional filters, there is no overlap between the blocks. For example, a $2{\\times}2{\\times}c$ block ($c$ bands) of the activation map my yield a $1{\\times}1{\\times}2$ output block.\n",
        "\n",
        "In the latter case, the output activation map is reduced to half its original size. This may be used to reduce the number of parameters in the upper layers and, at the same time, to further increase the receptive field.\n",
        "\n",
        "The most relevant parameters of a pooling layer are:\n",
        "* type of pool (maximum, average)\n",
        "* size of the pool; typically $2{\\times}2$\n",
        "* stride: number of pixels by which we move the pooling filter when sliding it along the input. Typically this value would be equal to the pool size.\n",
        "* padding: as with convolutional layers, indicates what to do with pixels outside of the image/activation map\n",
        "\n",
        "*Note*: you may notice that, if we use a padding of $1$ and use an _average_ pool, we are actually just adding a fixed convolutional layer with equal weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhgDU-fVx2Jv"
      },
      "source": [
        "\n",
        "### Excercise\n",
        "\n",
        "Do 2x2 max-pooling by hand, with a stride of 2, and \"VALID\" padding, on the following 2D input. What is the size of the output?\n",
        "\n",
        "\\begin{bmatrix}\n",
        "  9 & 5 & 4 & 5 & 6 & 4 \\\\\n",
        "  6 & 6 & 3 & 5 & 8 & 2 \\\\\n",
        "  4 & 6 & 9 & 1 & 3 & 6 \\\\\n",
        "  9 & 7 & 1 & 5 & 8 & 1 \\\\\n",
        "  4 & 9 & 9 & 5 & 7 & 3 \\\\\n",
        "  7 & 3 & 6 & 4 & 9 & 1 \n",
        "\\end{bmatrix}\n",
        "\n",
        "\n",
        "Reveal the cell below by double-clicking and running it, to check your answer when you're done. Notice how we use a Haiku Module to perform this task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1oIm3Nlb9zY"
      },
      "outputs": [],
      "source": [
        "#@title Answer { display-mode: \"form\" }\n",
        "#import numpy as np\n",
        "#import\n",
        "X = np.array([[9, 5, 4, 5, 6, 4],\n",
        "              [6, 6, 3, 5, 8, 2],\n",
        "              [4, 6, 9, 1, 3, 6],\n",
        "              [9, 7, 1, 5, 8, 1],\n",
        "              [4, 9, 9, 5, 7, 3],\n",
        "              [7, 3, 6, 4, 9, 1]])\n",
        "\n",
        "def max_pool_fun(x):\n",
        "  mp = hk.MaxPool(window_shape=(2, 2), strides=2, padding='VALID')\n",
        "  return mp(x)\n",
        "\n",
        "max_pool = hk.without_apply_rng(hk.transform(max_pool_fun))\n",
        "params = max_pool.init(None,X)\n",
        "print('pooled:\\n',max_pool.apply(params,X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Vsrgyudd2E"
      },
      "source": [
        "## The CIFAR10 Dataset\n",
        "Now that we understand convolutional, max-pooling and feed-forward layers, we can combine these as building blocks to build a ConvNet classifier for images. For this practical, we will use the colour image dataset CIFAR10 (pronounced \"seefar ten\") which consists of 50,000 training images and 10,000 test images. As we did in Practical 1, we take 10,000 images from the training set to form a validation set and visualise some example images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flWYFg3ydvMU"
      },
      "outputs": [],
      "source": [
        "cifar = tf.keras.datasets.cifar10\n",
        "(all_train_images, all_train_labels), (all_test_images, all_test_labels) = cifar.load_data()\n",
        "all_test_images = all_test_images.astype(float)*(1/255)\n",
        "all_train_images = all_train_images.astype(float)*(1/255)\n",
        "\n",
        "cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSzdYWpZd8RE"
      },
      "outputs": [],
      "source": [
        "# Take the last 10000 images from the training set to form a validation set\n",
        "ntrain = 10000\n",
        "ntest = 1000\n",
        "\n",
        "all_train_labels = all_train_labels.squeeze()\n",
        "train_images = all_train_images[:ntrain, :, :, :]\n",
        "train_labels = all_train_labels[:ntrain]\n",
        "\n",
        "all_test_labels = all_test_labels.squeeze()\n",
        "test_images = all_test_images[-ntest:, :, :, :]\n",
        "test_labels = all_test_labels[-ntest:]\n",
        "#\n",
        "# convert to JAX\n",
        "#\n",
        "X_train, X_test, Y_train, Y_test = jnp.array(train_images, dtype=jnp.float32),\\\n",
        "                                   jnp.array(test_images,  dtype=jnp.float32),\\\n",
        "                                   jnp.array(train_labels, dtype=jnp.float32),\\\n",
        "                                   jnp.array(test_labels,  dtype=jnp.float32)\n",
        "\n",
        "classes =  jnp.unique(Y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8jf1myGQP1O"
      },
      "source": [
        "What are the shapes and data-types of train_images and train_labels?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzLutGn-P7mg"
      },
      "outputs": [],
      "source": [
        "print('all_images.shape = {}, data-type = {}'.format(all_train_images.shape, all_train_images.dtype))\n",
        "print('all_labels.shape = {}, data-type = {}'.format(all_train_labels.shape, all_train_labels.dtype))\n",
        "print('train_images.shape = {}, data-type = {}'.format(X_train.shape, X_train.dtype))\n",
        "print('train_labels.shape = {}, data-type = {}'.format(Y_train.shape, Y_train.dtype))\n",
        "print('train_images.shape = {}, data-type = {}'.format(X_test.shape, X_test.dtype))\n",
        "print('train_labels.shape = {}, data-type = {}'.format(Y_test.shape, Y_test.dtype))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynwzGIAneBbb"
      },
      "source": [
        "### Visualise examples from the dataset\n",
        "Run the cell below multiple times to see various images. (They might look a bit blurry because we've blown up the small images.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nMTxCOjd9WW"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid('off')\n",
        "\n",
        "  img_index = np.random.randint(0, 10000)\n",
        "  plt.imshow(train_images[img_index])\n",
        "  plt.xlabel(cifar_labels[train_labels[img_index]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN-XUzp-fpyp"
      },
      "source": [
        "## A ConvNet Classifier\n",
        "Finally, we build a simple convolutional architecture to classify the CIFAR images. We will build a mini version of the AlexNet architecture, which consists of 5 convolutional layers with max-pooling, followed by 3 fully-connected layers at the end. In order to investigate the effect each of these two layers has on the number of parameters, we'll build the model in two stages. \n",
        "\n",
        "First, the convolutional layers + max-pooling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9zloewLws0b"
      },
      "outputs": [],
      "source": [
        "#@title Create Network, Haiku way\n",
        "\n",
        "#\n",
        "# Haiku module\n",
        "#\n",
        "class ConvNetModule(hk.Module):\n",
        "\n",
        "  def __init__(self):      \n",
        "    \"\"\"\n",
        "    add some layers to the net\n",
        "    \"\"\"\n",
        "    super().__init__(name=\"ConvNetModule\")\n",
        "    initializer  = hk.initializers.VarianceScaling(1.0,\"fan_avg\",\"truncated_normal\") # Glorot\n",
        "    self.conv1   = hk.Conv2D(output_channels=48, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.pool1   = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "\n",
        "    self.conv2   = hk.Conv2D(output_channels=128, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)\n",
        "    self.pool2   = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "    \n",
        "    self.conv3_1 = hk.Conv2D(output_channels=192, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.conv3_2 = hk.Conv2D(output_channels=192, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.conv3_3 = hk.Conv2D(output_channels=128, kernel_shape=(3,3), padding=\"SAME\",w_init=initializer)    \n",
        "    self.pool3 = hk.MaxPool(window_shape=(3,3),strides=(1,1),padding=\"SAME\")\n",
        "\n",
        "    self.flatten = hk.Flatten()\n",
        "    self.linear1 = hk.Linear(output_size=1024,w_init=initializer) # was 1024\n",
        "\n",
        "    self.linear2 = hk.Linear(output_size=1024,w_init=initializer) # was 1024, reduced due to OOM\n",
        "    self.linear3 = hk.Linear(output_size=len(classes),w_init=initializer)\n",
        "\n",
        "\n",
        "  def __call__(self, x_batch):\n",
        "    \"\"\"\n",
        "    apply the network to given inputs\n",
        "    \"\"\"\n",
        "\n",
        "    x = self.conv1(x_batch)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = jax.nn.relu(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.conv3_1(x)\n",
        "    x = self.conv3_2(x)\n",
        "    x = self.conv3_3(x)\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    x = self.linear1(x)\n",
        "    \n",
        "    coin = hk.next_rng_key()\n",
        "    x = hk.dropout(coin,0.5,x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = self.linear2(x)\n",
        "    x = jax.nn.relu(x)\n",
        "\n",
        "    x = self.linear3(x)\n",
        "    x = jax.nn.softmax(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "#\n",
        "# wrap it into a function\n",
        "#\n",
        "def cnn_fun(x):\n",
        "    cnn = ConvNetModule()\n",
        "    return cnn(x)\n",
        "\n",
        "#\n",
        "# transform into functional pair: init, apply\n",
        "#\n",
        "conv_net = hk.transform(cnn_fun) \n",
        "\n",
        "#\n",
        "# define the network loss\n",
        "#\n",
        "def cnn_loss(_params, _key, _X, _y_true):\n",
        "  _y_pred     = conv_net.apply(_params, _key, _X)\n",
        "  _y_true_hot = jax.nn.one_hot(_y_true, num_classes=len(classes))\n",
        "  _log_y_pred = jnp.log(_y_pred)\n",
        "  return -jnp.mean(_y_true_hot *_log_y_pred) # logit cost \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uzeJaMB7DPa"
      },
      "source": [
        "### Optional reading: initialization schemes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUBKcjfJ6gRs"
      },
      "source": [
        "You might have wondered what values we are using for the initial values of the weights and biases in our model. The short answer is that we typically use random initialization. In this case, we have just been using the default Keras initializers for each layer, which are usually sufficient.\n",
        "\n",
        "The longer answer is that just using completely random numbers does not always work best in practice and that there are a number of common initialization schemes (which are available in most deep learning frameworks such as TensorFlow and Keras).\n",
        "\n",
        "Let's consider a few examples:\n",
        "\n",
        " * When using the ReLU activation it is common to initialize the biases with small positive numbers because this encourages the ReLU activations to start off in the _on_ state, which helps to counteract the _dying ReLU problem_.\n",
        "\n",
        " * The deeper neural networks become the more likely it is that gradients will either shrink to the point that they vanish, or grow to the point that they overflow (the _vanishing_ and _exploding_ gradients problems). To help combat this we can initialize our weights to have a (model-specific) appropriate scale. One method for doing this is called [Xavier or Glorot](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) initialization.\n",
        "\n",
        " * The _Xavier_ initialization scheme was designed with the _traditional_ activations Sigmoid and TanH in mind and does not work as well for ReLU activations. An alternative is [He or Kaiming](https://arxiv.org/pdf/1502.01852.pdf) initialization which is a modification of Xavier initialization for ReLU activations.\n",
        "\n",
        " [This blog](http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) goes into more detail on _He_ and _Xavier_ initialization. [The Keras documentation](https://keras.io/initializers/) lists a number of common schemes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFrH0OVORco2"
      },
      "source": [
        "### Training and Validating the model\n",
        "In the last practical we wrote out the dataset pipeline, loss function and training-loop to give you a good appreciation for how it works. This time, we use the training loop built-in to Keras. For simple, standard datasets like CIFAR, doing it this way will work fine, but it's important to know what goes on under the hood because you may need to write some or all of the steps out manually when working with more complex datasets! "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def train_cnn_haiku(_train_data, _model, _loss_fun, \n",
        "                    _stepsize=1e-4,_epochs=25, _batch_size=100, \n",
        "                    _patience=1, _seed=42):\n",
        "  \"\"\"\n",
        "  optimization using Optax\n",
        "  :param _train_data: tuple (X_train,y_train,X_test,y_test)\n",
        "  :param _model: target model to be optimized\n",
        "  :param _loss_fun: loss function\n",
        "  :param _stepsize: how fast should we update parameters\n",
        "  :param _tolerance: if performance does not improve at least this ratio, stop\n",
        "  :param _batch_size: how many samples to evaluate at each batch\n",
        "  :param _seed: random seed. If you don't know why this is 42, read the Hitchhiker's Guide to the Galaxy by Douglas Adams\n",
        "  \"\"\"\n",
        "  _X_train, _y_train, _X_val, _y_val = _train_data\n",
        "  #\n",
        "  # create optimizer\n",
        "  # this guy defines how to update the parameters in each iteration\n",
        "  # try others! https://optax.readthedocs.io/en/latest/api.html\n",
        "  #\n",
        "  _optimizer = optax.adam(_stepsize)\n",
        "  #\n",
        "  # initialization\n",
        "  # note: handling random numbers in jax is delicate\n",
        "  # you have to carry the next key yourself\n",
        "  # see https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html\n",
        "  #\n",
        "  _key = jrand.PRNGKey(_seed)\n",
        "  _key, _subkey = jrand.split(_key) \n",
        "  #\n",
        "  # initial model parameters\n",
        "  #\n",
        "  _theta_t       = _model.init(_subkey, _X_train[:5,:])\n",
        "  #\n",
        "  # initial optimizer state\n",
        "  #\n",
        "  _state_t       = _optimizer.init(_theta_t)\n",
        "  \n",
        "  _nsamples = _X_train.shape[0]\n",
        "  _indexes = jnp.arange(_nsamples)\n",
        "  _nbatch = int(np.ceil(_nsamples / _batch_size))\n",
        "  _batches = list(_batch_size*i for i in range(_nbatch))\n",
        "  _batches.append(None) # tail\n",
        "\n",
        "  print('total number of samples:', _nsamples)\n",
        "  print('number of batches      :', _nbatch)\n",
        "  print('batch size             :', _batch_size)\n",
        "\n",
        "  _loss_history  = list()\n",
        "  _no_improvement = 0\n",
        "  #\n",
        "  # epochs\n",
        "  #\n",
        "  for t in range(_epochs):\n",
        "    #\n",
        "    # perform a number of batches\n",
        "    #\n",
        "    t0 = time.time()\n",
        "    _train_losses = list()\n",
        "    _val_losses   = list()\n",
        "    for i in range(_nbatch):\n",
        "      print('.',end='',flush=True)\n",
        "      _X_batch = _X_train[_batches[i]:_batches[i+1],:]\n",
        "      _y_batch = _y_train[_batches[i]:_batches[i+1]]\n",
        "      #del batch_idx\n",
        "      #\n",
        "      # current iterate\n",
        "      #\n",
        "      _key,_subkey = jrand.split(_key)\n",
        "      _train_loss_t, _grad_t  = jax.value_and_grad(_loss_fun)(_theta_t, _key, _X_batch, _y_batch)\n",
        "      _val_loss_t             = _loss_fun(_theta_t, _key, _X_val, _y_val)\n",
        "      _train_losses.append(_train_loss_t)\n",
        "      _val_losses.append(_val_loss_t)\n",
        "      del _X_batch,_y_batch\n",
        "      #\n",
        "      # optimization may go bananas for large stepsize\n",
        "      #\n",
        "      if np.isnan(_train_loss_t):\n",
        "        print(f\"training became unstable! Try using a step size smaller than {_stepsize}.\")\n",
        "        return None\n",
        "      #\n",
        "      # update iterate\n",
        "      #\n",
        "      _update_t, _state_t = _optimizer.update(_grad_t, _state_t) \n",
        "      del _grad_t\n",
        "      _theta_t = optax.apply_updates(_theta_t, _update_t)\n",
        "      del _update_t\n",
        "    #\n",
        "    # average loss throughout epoch\n",
        "    #\n",
        "    if t > 0:\n",
        "      _prev_val_loss = _mean_val_loss\n",
        "    _mean_train_loss = np.mean(np.array(_train_losses))\n",
        "    _std_train_loss  = np.std( np.array(_train_losses))\n",
        "    _mean_val_loss   = np.mean(np.array(_val_losses))\n",
        "    _std_val_loss    = np.std( np.array(_val_losses))\n",
        "    print(f'\\nepoch {t:3} | train loss {_mean_train_loss:6.4f}+/-{_std_train_loss:6.4f}',end=' | ')\n",
        "    print(f'val loss {_mean_val_loss:6.4f}+/-{_std_val_loss:6.4f}',end=' | ')\n",
        "    dt = time.time() - t0\n",
        "    print(f'batch time {dt:3.0f}')\n",
        "    _loss_history.append((_mean_train_loss,_std_train_loss,_mean_val_loss,_std_val_loss))\n",
        "    #\n",
        "    # evaluate improvement (or lack thereof)\n",
        "    #\n",
        "    # we don't count improvement if it falls below the width of the confidence \n",
        "    # interval of the validation loss in \"patience\" steps\n",
        "    #\n",
        "    if t > 0 and (_prev_val_loss-_mean_val_loss) < _std_val_loss/_patience:\n",
        "      _no_improvement += 1\n",
        "      if _no_improvement >= _patience:\n",
        "        print(\"no further improvement.\")\n",
        "        break\n",
        "    else: # reset patience\n",
        "      _no_improvement = 0\n",
        "    #\n",
        "    # update solution (here comes Optax)\n",
        "    #\n",
        "\n",
        "  return _theta_t,_loss_history\n",
        "\n",
        "#\n",
        "# run optimization\n",
        "#\n",
        "train_data = X_train, Y_train, X_test, Y_test\n",
        "theta,loss_history = train_cnn_haiku(train_data, conv_net, cnn_loss)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "Q83gS5PJUEDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlrSyMQpoV9f"
      },
      "source": [
        "## Try out our trained model\n",
        "\n",
        "We now use our trained model to classify a sample of 25 images from the test set. We pass these 25 images to the  `conv_net.apply` function, which returns a 25x10 matrix whose  $(i, j)$ entry contains the probability that the image $i$ belongs to class $j$. We obtain the most-likely prediction using the ```np.argmax``` function which returns the index of the maximum entry along the columns. Finally, we plot the result with the prediction and prediction probability labelled underneath the image and true label on the side. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjzP384wm9OW"
      },
      "outputs": [],
      "source": [
        "#@title Apply the model\n",
        "nsample    = 25\n",
        "rng        = jax.random.PRNGKey(42) # dummy; no random stuff hereafter\n",
        "Y_pred_hot = conv_net.apply(theta, rng, X_test[:nsample,:]) # apply our learned model\n",
        "Y_pred     = np.argmax(Y_pred_hot, axis=1) # prediction: index with highest probability\n",
        "Y_prob     = np.max(Y_pred_hot, axis=1) # retrieve the probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol-f9SacnySQ"
      },
      "outputs": [],
      "source": [
        "#@title Show the results\n",
        "plt.figure(figsize=(12,12))\n",
        "for i in range(nsample):\n",
        "  img = X_test[i,:]\n",
        "  pred_class = Y_pred[i]\n",
        "  pred_label = cifar_labels[pred_class]\n",
        "  pred_prob  = Y_prob[i]\n",
        "  true_class = int(Y_test[i])\n",
        "  true_label = cifar_labels[true_class]\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid('off')\n",
        "\n",
        "  plt.imshow(img)\n",
        "  # draw green or red dots to see if we hit or miss\n",
        "  plt.scatter(2,2,s=140,color=\"white\")\n",
        "  if true_class == pred_class:\n",
        "    plt.scatter(2,2,s=100,color=\"green\")\n",
        "  else:\n",
        "    plt.scatter(2,2,s=100,color=\"red\")\n",
        "  plt.xlabel('{} ({:0.2f})'.format(pred_label, pred_prob))\n",
        "  plt.ylabel('{}'.format(true_label))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0618tOoMv0Y"
      },
      "source": [
        "### Question\n",
        "What do you think of the model's predictions? Looking at the model's confidence (the probability assigned to the predicted class), look for examples of the following cases:\n",
        "1. The model was correct with high confidence\n",
        "2. The model was correct with low confidence\n",
        "3. The model was incorrect with high confidence\n",
        "4. The model was incorrect with low confidence\n",
        "\n",
        "What do you think the (relative) loss values would be in those cases? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py0V6UwC6_kH"
      },
      "source": [
        "### Optional extra reading: Uncertainty in deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpGl8VzY6B3c"
      },
      "source": [
        "Deep neural networks are not considered to be very good at estimating the uncertainty in their predictions. However, knowing your model's uncertainty can be very important for many applications. For example, consider a deep learning tool for diagnosing diseases, in this case, a false negative could have massive impacts on a person's life! We would really like to know how confident our model is in its prediction. This is a budding field of research, for example, see [this blog](https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html) for a nice introduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJzCooQO66D3"
      },
      "source": [
        "### Optional extra reading: CNN architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9Sjwpsm5_TD"
      },
      "source": [
        "Deciding on the architecture for a CNN, i.e. the combination of convolution, pooling, dense, and other layers, can be tricky and often can seem arbitrary. On top of that, one also has to make decisions such as what kind of pooling, which activation functions, and what size of convolution to use, among other things. For new and old practitioners of deep learning, these choices can be overwhelming. \n",
        "\n",
        "However, by examining existing successful CNN architectures we can learn a lot about what works and what doesn't. (We can even apply these existing architectures to our problems since many deep learning libraries, such as TensorFlow and Keras, have them [built in](https://keras.io/applications/#available-models) and it is even possible to fine-tune pre-trained models to our specific problem using [transfer learning](https://cs231n.github.io/transfer-learning/).)\n",
        "\n",
        "[This article](https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5) describes many of the most successful CNN architectures in recent years, including [ResNet](https://arxiv.org/abs/1512.03385), [Inception](https://arxiv.org/pdf/1512.00567v3.pdf) and [VGG](https://arxiv.org/pdf/1409.1556.pdf). For a more detailed and technical description of these models and more see [these slides](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf). Reading through these resources should give you insights into why these architectures are successful as well as best practices and current trends for CNNs that will help you design your own architectures.\n",
        "\n",
        "For example, one of the practices you might pick up on is the use of 3x3 convolutions. You'll notices that older architectures such as [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) used a range of convolutions from 7x7 down to 3x3. However, newer architectures such as VGG and ResNet use 3x3 convolutions almost exclusively. In short, the reason is that stacking 3x3 convolutions gives you the same receptive field as a larger convolution but with more non-linearity. \n",
        "\n",
        "Here are some other questions you may want to think about while investigating these architectures:\n",
        "\n",
        "* Why do modern architectures use less max-pooling?\n",
        "* What does a 1x1 convolution do?\n",
        "* What is a residual connection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3bIU8BErhiJ"
      },
      "source": [
        "## Your Tasks\n",
        "1. [**ALL**] Experiment with the network architecture, try changing the numbers, types and sizes of layers, the sizes of filters, using different padding etc. How do these decisions affect the performance of the model? In particular, try building a *fully convolutinoal* network, with no (max-)pooling layers. \n",
        "2. [**ALL**] Add BATCH NORMALISATION ([Tensorflow documentation](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)) to improve the model's generalisation.\n",
        "3. [**ADVANCED**] Read about Residual networks ([original paper](https://arxiv.org/pdf/1512.03385.pdf), ) and add **shortcut connections** to the model architecture. Try to build a simple reusable \"residual block\" as a [Keras Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model). \n",
        "4. [**OPTIONAL**]. Visualise the filters of the convolutional layers using Matplotlib. **HINT**: You can retrieve a reference to an individual layer from the sequential Keras model by calling```model.get_layer(name)```, replacing \"name\" with the name of the layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHbjtjTZz7Q"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e-IdtqUknDK"
      },
      "source": [
        "##Additional Resources\n",
        "\n",
        "Here's some more information on ConvNets:\n",
        "\n",
        "* Chris Colah's blog post on [Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
        "* [How do convolutional neural networks work?](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\n",
        "* The [CS231n course](https://cs231n.github.io/) which is a great resource that covers just about all the basics of CNNs\n",
        "* [Building blocks of interpretability](https://distill.pub/2018/building-blocks/) (some really cool CNN feature visualisations)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "# **Feedback**\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-tOweUjT5Kd"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/AjhUiYxfLFV5gMGa7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<center><img src=\"https://github.com/khipu-ai/global-resources/raw/main/logo/khipu.png\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7Wuh9qRkE1vs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}