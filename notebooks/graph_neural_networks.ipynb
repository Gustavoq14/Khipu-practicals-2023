{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "30414242",
      "metadata": {
        "id": "30414242"
      },
      "source": [
        "# Collaborative Filtering with Graph Neural Networks\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/khipu-ai/practicals-2023/blob/main/notebooks/graph_neural_networks.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "### Copyright\n",
        "\n",
        "* Khipu 2023. Apache License 2.0.\n",
        "\n",
        "### Authors\n",
        "\n",
        "Juan Elenter, Tatiana Guevara, Ignacio Hounie,\n",
        "Charilaos Kanatsoulis, Ignacio Ramírez and Alejandro Ribeiro*\n",
        "\n",
        "*Alphabetical order.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The objective of this lab is to design a recommendation system that predicts the ratings that customers would give to a certain product. Say, the rating that a moviegoer would give to a specific movie, or the rating that an online shopper would give to a particular offering. To make these predictions we leverage the ratings that customers have given to this or similar products in the past. This approach to ratings predictions is called collaborative filtering. \n",
        "\n",
        "We model collaborative filtering as a problem that involves a graph $\\mathbf{S}$ and graph signals $\\mathbf{x}_u$ supported on the nodes of the graph. \n",
        "Nodes $p$ of the graph $\\mathbf{S}$ represent different products and the weighted edges $S_{pq}$ denote simlarities between products $p$ and $q$. The entries of the graph signal $\\mathbf{x}_u$ represent the ratings that user $u$ has given to each product. \n",
        "\n",
        "The motivation for developing a recommendation system is that customers have rated a subset of prodcuts. Each of us has seen a small number of movies or bought a small number of offerings. Thus, the ratings vector $\\mathbf{x}_u$ contains only some entries that correspond to rated products. For the remaining entries we adopt the convention that $x_{ui}=0$. Our goal is to learn a function $\\Phi(\\mathbf{x}_{u}; \\mathcal{H})$ that takes as input the vector $\\mathbf{x}_u$ of available ratings and fills in predictions for the ratings that are not available. \n",
        "\n",
        "We will develop and evaluate a graph neural network (GNN) for making these rating predictions.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This practical assumes that you are familiar with:\n",
        "* Python, JAX and Flax\n",
        "* Basic object oriented programming\n",
        "* Basic optimization (gradient descent, etc.)\n",
        "\n",
        "## Hardware requirements\n",
        "\n",
        "* You don't need to use a GPU (training should take minutes). However, if you want to speed up training, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "207a46e5",
      "metadata": {
        "id": "207a46e5"
      },
      "source": [
        "\n",
        "## Environment setup (Run cell)\n",
        "\n",
        "Before we beging we need to import (and install) the necessary Python Packages. We use Numpy to load data, Jax for training, and matplotlib to plot and visualize results.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi==0.17.2\n",
        "!pip install --upgrade -q git+https://github.com/google/flax.git"
      ],
      "metadata": {
        "id": "9kj56L8rmim8"
      },
      "id": "9kj56L8rmim8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec066d2",
      "metadata": {
        "id": "fec066d2",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from flax import linen as nn\n",
        "from jax import lax, random, numpy as jnp\n",
        "import copy\n",
        "import pickle as pkl\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import optax\n",
        "import tqdm\n",
        "from tqdm import tqdm\n",
        "import jax\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# PART 1 - Collaborative Filtering\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "g-uxBmvsMogb"
      },
      "id": "g-uxBmvsMogb"
    },
    {
      "cell_type": "markdown",
      "id": "c6f4b8b0",
      "metadata": {
        "id": "c6f4b8b0"
      },
      "source": [
        "\n",
        "\n",
        "## Movie Rating Data: Graph, Inputs, and Outputs\n",
        "\n",
        "As a specific example, we use the MovieLens-100k dataset. The MovieLens-100k dataset consists of ratings given by $U$ users to $P$ movies (products). The existing movie ratings are integer values between 1 and 5. Therefore, the data are represented by a $U\\times P$ matrix $X$ where $x_{up}$ is the rating that user $u$ gives to movie $p$. If user $u$ has not rated movie $p$, then $x_{up} = 0$. We see that each row of this matrix corresponds to a vector of ratings $\\mathbf{x}_u$ of a particular user.\n",
        "\n",
        "To build the collaborative filtering system, we use the rating history of all movies to compute a graph of movie similarities. In this graph edges are crosscorrelations of movie ratings. The graph can be constructed from the raw $U\\times P$ movie rating matrix, but to make matters simpler we have constructed this graph already and are making it available as part of the dataset. \n",
        "\n",
        "\n",
        "To train this collaborative filtering system we use rating histories to create a dataset with entries $(\\mathbf{x}_n, y_n, p_n)$. In these entries $\\mathbf{x}_n$ is a vector that contains the ratings of a particular user, $y_n$ is a scalar that contains a rating that we want to predict, and $p_n$ is the index of the movie (product) that corresponds to the rating $y_n$. To evaluate this collaborative filtering system we use rating histories to create a dataset with entries having the same format. Both of these datasets can be constructed from the raw $U\\times P$ movie rating matrix, but to make matters simpler we have constructed them already and are making them available as part of the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 Dataset set-up\n",
        "\n",
        "*   Step 1: Download the data from [this link](https://drive.google.com/file/d/10qoedde9D6vrnv8HO_q38khGHfcD8pk_/view?usp=share_link).\n",
        "*   Step 2: Upload the file 'movie_data_numpy.p' to this Colab Session by clicking on Files>Upload in the left-side menu."
      ],
      "metadata": {
        "id": "PrJmTORLrxyf"
      },
      "id": "PrJmTORLrxyf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3492179e",
      "metadata": {
        "id": "3492179e"
      },
      "outputs": [],
      "source": [
        "# The following command loads the data. For this to work the file \"movie_data.p\" \n",
        "# must be uploaded to this notebook. To do that, navigate to the Jupyter Notebook \n",
        "# interface home page and use the “Upload” option. \n",
        "\n",
        "data = pkl.load(open(\"movie_data_numpy.p\",\"rb\"))\n",
        "\n",
        "# The data file contains a graph of movie similarities. The graph has P nodes.\n",
        "# This is the graph we will use to run the GNN.\n",
        "\n",
        "S = data['S']\n",
        "P = S.shape[0]\n",
        "\n",
        "# The data file contains a training set with entries (x_n, y_n, p_n).\n",
        "# These entries are stored in the arrays xTrain, yTrain, and pTrain,\n",
        "# respectively. All of these arrays have nTrain columns, with each \n",
        "# column corresponding to a different entry in the dataset.\n",
        "\n",
        "xTrain = data['train']['x'] # Available ratings for making predictions. \n",
        "yTrain = data['train']['y'] # Rating to be predicted\n",
        "pTrain = data['train']['p'] # Index of the movie whose rating is being predicted\n",
        "nTrain = xTrain.shape[0]\n",
        "\n",
        "\n",
        "# Same structure described above with test data\n",
        "\n",
        "xTest = data['test']['x'] \n",
        "yTest = data['test']['y'] \n",
        "pTest = data['test']['p'] \n",
        "nTest = xTest.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00c396da",
      "metadata": {
        "id": "00c396da"
      },
      "source": [
        "\n",
        "## Movie Similarity Graph\n",
        "\n",
        "To help visualize the movie recommendation problem it is instructive to plot the movie similarity network $\\mathbf{S}$; this is matrix of weights where each of the entries represents a similarity score between different movies.\n",
        "\n",
        "In the figure below each bright dot corresponds to a large entry $S(p,q)$. This denotes a pair of movies that watchers tend give similar scores to. For instance, say that when someone scores \"Star Wars IV\" highly, they are likely to score \"Star Wars V\" highly and that the converse is also true; poor scores in one correlate with poor scores in the other. The entry $S(\\text{\"Star Wars IV\"}, \\text{\"Star Wars V\"})$ is large.\n",
        "\n",
        "Fainter entries $S(p,q)$ denote pairs of movies with less socre correlation. Perhaps between \"Star Wars\" and \"Star Trek\" which have overlapping but not identical fan bases. Dark entries $S(p,q)$ correspond to pairs movies with no correlation between audience scores. Say when $p$ is the index of \"Star Wars\" and $q$ is the index of \"Little Miss Sunshine.\"\n",
        "\n",
        "(Run Cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88ceac98",
      "metadata": {
        "id": "88ceac98"
      },
      "outputs": [],
      "source": [
        "plt.figure(1, figsize = (8,8))\n",
        "plt.title('Pattern of the Movie Recommendation Adjacency Matrix')\n",
        "plt.xlabel('Movie index (p)')\n",
        "plt.ylabel('Movieindex (q)')\n",
        "plt.imshow(S)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9efaea5c",
      "metadata": {
        "id": "9efaea5c"
      },
      "source": [
        "\n",
        "## Mean Squared Error for Training and Testing\n",
        "\n",
        "If we have a function $\\hat{\\mathbf{y}}_n=\\Phi(\\mathbf{x}_{n}; \\mathcal{H})$ that makes rating predictions out of availbale ratings, we can evaluate the goodness of this function with the squared loss\n",
        "\n",
        "\\begin{align}\n",
        "   \\ell \\big(\\Phi(\\mathbf{x}_{n}; \\mathcal{H}), y_{n} \\big)\n",
        "      ~=~ \n",
        "               \\Big[ \\big(\\hat{\\mathbf{y}}_n \\big)_{p_n} - y_n \\Big]^2\n",
        "      ~=~ \n",
        "               \\Big[ \\big( \\Phi(\\mathbf{x}_{n}; \\mathcal{H}) \\big)_{p_n} - y_n \\Big]^2.\n",
        "\\end{align}\n",
        "\n",
        "Notice that in this expression the function $\\hat{\\mathbf{y}}_n=\\Phi(\\mathbf{x}_{n}; \\mathcal{H})$ makes predictions for all movies. However, we isolate entry $p_n$ and compare it against the rating $y_n$. We do this, because the rating $y_n$ of movie $p_n$ is the one we have available in the training or test sets.\n",
        "\n",
        "We remark the fact that the function $\\hat{\\mathbf{y}}=\\Phi(\\mathbf{x}; \\mathcal{H})$ makes predictions for all movies is important during operation. The idea of the recommendation system is to identify the subset of products that the customer would rate highly. They are the ones that we will recommend. This is why we want a system that has a graph signal as an output even though the available dataset has scalar outputs.\n",
        "\n",
        "## Task 2: Write a function to evaluate the training loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24bb1929",
      "metadata": {
        "id": "24bb1929"
      },
      "outputs": [],
      "source": [
        "\n",
        "def movieMSELoss(yHat, y, idxMovie):\n",
        "\n",
        "    '''\n",
        "    Movie MSE loss function. This function evaluates the square of comparing yHat(p) with y. The function \n",
        "    is overloaded to accept as an input tensors with multiple movie ratings. If\n",
        "    given multiple movie ratings it computes the mean squared error (the error\n",
        "    averaged over the dataset)\n",
        "    \n",
        "    Inputs:\n",
        "      yHat: jnp.ndarray\n",
        "        A set of rating estimates. It includes estimates of all movies.\n",
        "      y: jnp.ndarray\n",
        "        A specific rating of a specific movie\n",
        "      idxMovie: int, jnp.ndarray\n",
        "        The index of the movie whose rating is given by y\n",
        "\n",
        "    Outputs:\n",
        "      mse: jnp.ndarray\n",
        "        Computed mean squared error.\n",
        "    \n",
        "    '''\n",
        "    ## implement it\n",
        "    return #mse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer"
      ],
      "metadata": {
        "id": "pP_ko1OI9EWz"
      },
      "id": "pP_ko1OI9EWz"
    },
    {
      "cell_type": "code",
      "source": [
        "def movieMSELoss(yHat, y, idxMovie):\n",
        "\n",
        "    '''\n",
        "    Movie MSE loss function. This function evaluates the square of comparing yHat(p) with y. The function \n",
        "    is overloaded to accept as an input tensors with multiple movie ratings. If\n",
        "    given multiple movie ratings it computes the mean squared error (the error\n",
        "    averaged over the dataset)\n",
        "    \n",
        "    Inputs:\n",
        "      yHat: jnp.ndarray\n",
        "        A set of rating estimates. It includes estimates of all movies.\n",
        "      y: jnp.ndarray\n",
        "        A specific rating of a specific movie\n",
        "      idxMovie: int, jnp.ndarray\n",
        "        The index of the movie whose rating is given by y\n",
        "\n",
        "    Outputs:\n",
        "      mse: jnp.ndarray\n",
        "        Computed mean squared error.\n",
        "    \n",
        "    '''\n",
        "    # the .squeeze() method is needed for dimension match between y and yHat\n",
        "    yHat = yHat.squeeze()\n",
        "    # Isolate the predictions in yHat that we will use for comparison\n",
        "    pred = yHat[jnp.arange(y.shape[0]), idxMovie]\n",
        "    # Evaluate mean squared error\n",
        "    mse = jnp.mean((pred-y)**2)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "AIzjKW0l9Dev"
      },
      "id": "AIzjKW0l9Dev",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "# PART 2 - Graph Convollutions\n",
        "\n",
        "<hr>"
      ],
      "metadata": {
        "id": "X6U5U3BJMx5g"
      },
      "id": "X6U5U3BJMx5g"
    },
    {
      "cell_type": "markdown",
      "id": "89876185",
      "metadata": {
        "id": "89876185"
      },
      "source": [
        "\n",
        "## Graph Filter\n",
        "\n",
        "One convenient model for processing graph signals is a graph _convolutional filter_. To define this operation we introduce a filter of order $K$ along with filter coefficients $\\mathbf{H}_k$ that we group in the tensor $\\mathcal{H}=[\\mathbf{H}_0,\\ldots, \\mathbf{H}_{K-1}]$. A graph convolutional filter applied to the graph signal $\\mathbf{X}$ is a set of polynomials in the graph shift operator (GSO) $\\mathbf{S}$, i.e.,\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{\\Phi}(\\mathbf{X}; \\mathbf{h}, \\mathbf{S}) = \\sum_{k=0}^{K-1} \\mathbf{S}^k \\mathbf{X}\\, \\mathbf{H}_k\n",
        "\\end{equation}\n",
        "\n",
        "where the output $\\mathbf{\\Phi} (\\mathbf{X}; \\mathcal{H}, \\mathbf{S})$ is also a graph signal and the learnable parameters $\\mathcal{H}$ are the filter coefficients $\\mathbf{H}_k$.\n",
        "\n",
        "One advantage of graph filters is their locality. Indeed, we can define the diffusion sequence as the collection of graph signals $\\mathbf{U}_k = \\mathbf{S}^k \\mathbf{X}$ to rewrite the filter above as $\\mathbf{U} = \\sum_{k=0}^{K-1} \\mathbf{U}_k \\mathbf{H}_k$. It is easy to see that the diffusion sequence is given by the recursion $\\mathbf{U}_k = \\mathbf{S}\\mathbf{U}_{k-1}$ with $\\mathbf{U}_0=\\mathbf{X}$. Further observing that $S_{ij}\\neq 0$ only when the pair $(i,j)$ is an edge of the graph, we see that the rows of the diffusion sequence satisfy\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{U}_{k}[i] \\ = \\ \\sum_{j:(i,j)\\in\\mathcal{E}} S_{ij} \\mathbf{U}_{k-1}[j],\n",
        "\\end{equation}\n",
        "\n",
        "where $\\mathcal{E}$ is the edge set of the graph. We can therefore interpret the graph filter as an operation that propagates information through adjacent nodes. This is a property that graph convolutional filters share with regular convolutional filters in time and offers motivation for their use in the processing of graph signals.\n",
        "\n",
        "To implement a graph filter we need two components: a function that implements the shift-and-sum operation and a class that defines the graph filter as a learning architecture. Given the filter coefficients, the graph shift operator and the input graph signal, we can then implement a class for the graph filter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "\n",
        "Write a function that implements a graph filter. This function\n",
        "takes as inputs the shift operator S, the filter coefficients Hk and the input\n",
        "signal X. To further improve practical performance we add a bias term B\n",
        "to the filter operation."
      ],
      "metadata": {
        "id": "TLY3lCXP_CQh"
      },
      "id": "TLY3lCXP_CQh"
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_filter(x, h, S, b):\n",
        "    '''\n",
        "    The following function defines a Graph Filter. \n",
        "    Inputs: \n",
        "      x: Input to Graph Filter (jnp.ndarray with dimensions B x G x N)\n",
        "      h: Filter (jnp.ndarray with dimensions K x G x F)\n",
        "      S: Graph Shift Operator (jnp.ndarray with dimensions N x N)\n",
        "      b: Bias term (jnp.ndarray with dimensions F)\n",
        "\n",
        "      Outpus:\n",
        "      y: Output of Graph Filter\n",
        "    '''\n",
        "    # Implement convolution\n",
        "\n",
        "    return #y\n",
        "    "
      ],
      "metadata": {
        "id": "jLdYstIt_Hw2"
      },
      "id": "jLdYstIt_Hw2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer"
      ],
      "metadata": {
        "id": "sQXGFjWL_lAw"
      },
      "id": "sQXGFjWL_lAw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613db5d1",
      "metadata": {
        "id": "613db5d1"
      },
      "outputs": [],
      "source": [
        "def graph_filter(x, h, S, b):\n",
        "    '''\n",
        "    The following function defines a Graph Filter. \n",
        "    Inputs: \n",
        "      x: Input to Graph Filter\n",
        "      h: Filter\n",
        "      S: Graph Shift Operator\n",
        "      b: Bias term\n",
        "\n",
        "      Outpus:\n",
        "      y: Output of Graph Filter\n",
        "    '''\n",
        "    # X is  B x G x N\n",
        "    # S is NxN\n",
        "    B, G, N = x.shape\n",
        "    K, _, F = h.shape\n",
        "    y = jnp.zeros((B, N, F))\n",
        "    # The following for-loop is utilized to perform the graph diffusions\n",
        "    for k in range(K+1):\n",
        "        # sum S^k x * h_k\n",
        "        # We transpose x it to get dimensions B x N x G\n",
        "        # The filter has dimensions G x F\n",
        "        y += jnp.matmul(x.transpose((0,2,1)), h[k])\n",
        "        # diffuse signal\n",
        "        x = jnp.matmul(x, S)\n",
        "    # y has dims B x N x F, \n",
        "    # add bias of shape F\n",
        "    y = y + b\n",
        "    # transpose it to get the desired dimensions B x F x N\n",
        "    y = y.transpose(0, 2, 1)\n",
        "    return  y\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fb0c216",
      "metadata": {
        "id": "1fb0c216"
      },
      "source": [
        "# Initialisation (Run Cell)\n",
        "\n",
        "Before starting the training procedure, we need to initialize the parameters of the graph filter. Specifically, the filter weights and bias are sampled from a uniform distribution with support in $[ -\\frac{1}{\\sqrt{K F_{in}}}, \\frac{1}{\\sqrt{K F_{in}}} ]$ with K being the order of the filter and $F_{in}$ the input dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4404a64",
      "metadata": {
        "id": "e4404a64"
      },
      "outputs": [],
      "source": [
        "\n",
        "def init_graph_filter(rng_key, shape):\n",
        "    \"\"\"\n",
        "    Initialise filters coefficients using a uniform distribution.\n",
        "\n",
        "    Input: \n",
        "    rng_key: jnp.ndarray\n",
        "      Random number generator key\n",
        "    shape: Union(tuple, list)\n",
        "      [k, f_in, f_out] with:\n",
        "        k: int order of the filter\n",
        "        f_in: int filter input dimension\n",
        "        f_out:  int filter output dimension\n",
        "\n",
        "    Output: jnp.ndarray \n",
        "      Randomly initialized filter matrix\n",
        "    \"\"\"\n",
        "    (k, f_in, f_out) = shape\n",
        "    stdv = 1. / math.sqrt(f_in * k)\n",
        "   \n",
        "    return random.uniform(rng_key, shape=(k, f_in, f_out),\n",
        "                          minval=-stdv, maxval=stdv)\n",
        "    \n",
        "def init_bias(rng_key, shape):\n",
        "    \"\"\"\n",
        "    Initialise bias coefficients using a uniform distribution\n",
        "\n",
        "    Input:\n",
        "    rng_key: jnp.ndarray \n",
        "      random number generator key\n",
        "    shape: Union(tuple, list) \n",
        "      [k, f_in, f_out] with:\n",
        "      k: int order of the filter\n",
        "      f_in: int ilter input dimension\n",
        "      f_out:  int filter output dimension\n",
        "    \n",
        "    Output: jnp.ndarray \n",
        "      Randomly initialized bias vector\n",
        "    \"\"\"\n",
        "    (k, f_in, f_out) = shape\n",
        "    stdv = 1. / math.sqrt(f_in * k)\n",
        "   \n",
        "    return random.uniform(rng_key, shape=(f_out,), minval=-stdv, maxval=stdv)        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flax nn.Module (Run cell)\n",
        "\n",
        "We wrap the graph filter in a Flax Module"
      ],
      "metadata": {
        "id": "Bt0XwpsRGZkZ"
      },
      "id": "Bt0XwpsRGZkZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7adda6a9",
      "metadata": {
        "id": "7adda6a9"
      },
      "outputs": [],
      "source": [
        "# We wrap the graph filter in a Flax Module\n",
        "\n",
        "class FilterModule(nn.Module):\n",
        "    k: int #order of the filter\n",
        "    f_in: int # filter input dimension\n",
        "    f_out: int # filter output dimension\n",
        "    \n",
        "    # setup method gets called on init\n",
        "    def setup(self):\n",
        "        # declare parameters\n",
        "        # conv filters\n",
        "        self.h = self.param('h',# parameter key \n",
        "                            init_graph_filter, # Initialization function\n",
        "                            (self.k, self.f_in, self.f_out))  # shape info.\n",
        "        # bias added to each output filter\n",
        "        self.b = self.param('b',# parameter key \n",
        "                            init_bias, # Initialization function\n",
        "                            (self.k, self.f_in, self.f_out))  # shape info.\n",
        "    \n",
        "    # Define the forward pass\n",
        "    def __call__(self, x, S):\n",
        "        return graph_filter(x, self.h, S, self.b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing predictions and evaluating the loss (Run Cells)\n",
        "\n",
        "We will also define a function that, given a model object and parameters, computes predictions and evaluates the loss."
      ],
      "metadata": {
        "id": "N3AzZ-xt8wJt"
      },
      "id": "N3AzZ-xt8wJt"
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_loss(params, model, x, s, y, p):\n",
        "    \"\"\"\n",
        "    Function that performas a forward pass (i.e: computes predictions) of a flax modeel and \n",
        "    evaluates mean squared error.\n",
        "\n",
        "    Input:\n",
        "      params: flax.core.FrozenDict\n",
        "         model parameters\n",
        "      x: jnp.ndarray \n",
        "        input graph signal \n",
        "      y: jnp.ndarray\n",
        "         output (label) graph signal\n",
        "      p: jnp.ndarray\n",
        "         target movies\n",
        "    Output:\n",
        "      mse jnp.ndarray:\n",
        "        loss\n",
        "    \"\"\"\n",
        "    yHat = model.apply(params, x, s)\n",
        "    return movieMSELoss(yHat, y, p)\n"
      ],
      "metadata": {
        "id": "20a9yzD585hu"
      },
      "id": "20a9yzD585hu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following class wraps the Filter Module class defined above in an nn.Module and adds a readout layer. This will allow us ot use a graph filter for the movie rating prediction task."
      ],
      "metadata": {
        "id": "XJiDEg0xiP6q"
      },
      "id": "XJiDEg0xiP6q"
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphFilter(nn.Module):\n",
        "\n",
        "    k: int  # order of filters\n",
        "    f: int # number of filters \n",
        "    \n",
        "    # setup method gets called on init\n",
        "\n",
        "    def setup(self):\n",
        "        self.gml = FilterModule(self.k, 1, self.f)\n",
        "        # add last linear layer\n",
        "        self.readout = FilterModule(1, self.f, 1)\n",
        "    \n",
        "    # Define the forward pass\n",
        "    def __call__(self, x, S):\n",
        "\n",
        "        x = self.gml(x, S)\n",
        "        # linear readout\n",
        "        x = self.readout(x, S)\n",
        "        return x"
      ],
      "metadata": {
        "id": "YdidBwmffWPD"
      },
      "id": "YdidBwmffWPD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate Graph filter (Run Cell)\n",
        "We now instantiate a Graph filter with the following properties:\n",
        "\n",
        "*   Filter order: 5\n",
        "*   Output Features: 64\n"
      ],
      "metadata": {
        "id": "JAMaVt73hSEs"
      },
      "id": "JAMaVt73hSEs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate Graph filter of order 5 with 64 output features and a readout layer.\n",
        "k = 5\n",
        "f = 64\n",
        "\n",
        "model = GraphFilter(k, f)"
      ],
      "metadata": {
        "id": "IkUCJE0lfWTb"
      },
      "id": "IkUCJE0lfWTb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Graph Filter"
      ],
      "metadata": {
        "id": "ajROvsOtfXIz"
      },
      "id": "ajROvsOtfXIz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training method and parameters\n",
        "\n",
        "Below we define the training parameters and specify the optimiewr. The training procedure has the following charasteristics:\n",
        "\n",
        "*   Number of epochs (full passes over the dataset): 8\n",
        "*   Initial Learning Rate: 0.01\n",
        "*   Optimizer: ADAM\n",
        "*   Batch Size: 20"
      ],
      "metadata": {
        "id": "4NZxPFDmf2QA"
      },
      "id": "4NZxPFDmf2QA"
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "nEpochs = 8\n",
        "learningRate = 0.01\n",
        "batchSize = 20\n",
        "\n",
        "# create a random key for init\n",
        "key1 = random.PRNGKey(0)\n",
        "\n",
        "# we need to pass thw random key and\n",
        "# data inputs to call to the initialisation\n",
        "# bc it computes a forward pass\n",
        "params =  model.init(key1, xTrain, S)\n",
        "\n",
        "# optimizer\n",
        "tx = optax.adam(learning_rate=learningRate)\n",
        "opt_state = tx.init(params)\n",
        "\n",
        "#  forward and loss\n",
        "loss_grad_fn = jax.value_and_grad(predict_and_loss)"
      ],
      "metadata": {
        "id": "ZSBL-T4ifWWR"
      },
      "id": "ZSBL-T4ifWWR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper variables for data loading during training.\n",
        "batchIndex = np.append(np.arange(0, nTrain, batchSize), nTrain)\n",
        "nBatches = int(np.ceil(nTrain/batchSize))"
      ],
      "metadata": {
        "id": "I2x9wn3Afv2O"
      },
      "id": "I2x9wn3Afv2O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop\n",
        "\n",
        "Below is the main trainig loop implementation. This is a very straightforward implementation based on a fixed number of epochs. \n",
        "For each batch, the following steps are performed:\n",
        "\n",
        "*   Compute prediction\n",
        "*   Evaluate loss\n",
        "*   Compute gradient of loss wrt parameters.\n",
        "*   Update parameters\n",
        "\n",
        "## TASK 4\n",
        "\n",
        "Train a graph filter to predict movie ratings. Plot the evolution of the training loss and\n",
        "evaluate the loss in the test dataset."
      ],
      "metadata": {
        "id": "o_YQKqQ-hmym"
      },
      "id": "o_YQKqQ-hmym"
    },
    {
      "cell_type": "code",
      "source": [
        "epoch = 0 # epoch counter\n",
        "lossTrain = []\n",
        "for epoch in range(nEpochs):\n",
        "    # permute samples\n",
        "    randomPermutation = np.random.permutation(nTrain)\n",
        "    idxEpoch = [int(i) for i in randomPermutation]\n",
        "    epoch_loss = 0.\n",
        "    for batch in tqdm(range(nBatches)):\n",
        "        # Determine batch indices\n",
        "        thisBatchIndices = idxEpoch[batchIndex[batch]: batchIndex[batch+1]]\n",
        "        # Get the samples in this batch\n",
        "        xTrainBatch = xTrain[thisBatchIndices,:,:]\n",
        "        yTrainBatch = yTrain[thisBatchIndices]\n",
        "        pTrainBatch = pTrain[thisBatchIndices]\n",
        "        ## compute loss and gradients using loss_grad_fn\n",
        "        # loss, grads = ## IMPLEMENT\n",
        "        ## optimizer step using tx.update\n",
        "        # updates, opt_state = ## IMPLEMENT\n",
        "        ## update parameters using optax.apply_updates\n",
        "        # params = ## IMPLEMENT\n",
        "        ## append loss for plotting\n",
        "        #lossTrain.append(loss)\n",
        "        #epoch_loss += loss\n",
        "    #print(f\"Epoch: {epoch+1} \\t Loss {epoch_loss/nBatches:.2f} \\n\")\n"
      ],
      "metadata": {
        "id": "Fg1_sW-XAYP-"
      },
      "id": "Fg1_sW-XAYP-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ],
      "metadata": {
        "id": "deJCl4slBCu3"
      },
      "id": "deJCl4slBCu3"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epoch = 0 # epoch counter\n",
        "lossTrain = []\n",
        "for epoch in range(nEpochs):\n",
        "    # permute samples\n",
        "    randomPermutation = np.random.permutation(nTrain)\n",
        "    idxEpoch = [int(i) for i in randomPermutation]\n",
        "    epoch_loss = 0.\n",
        "    for batch in tqdm(range(nBatches)):\n",
        "        # Determine batch indices\n",
        "        thisBatchIndices = idxEpoch[batchIndex[batch]: batchIndex[batch+1]]\n",
        "        # Get the samples in this batch\n",
        "        xTrainBatch = xTrain[thisBatchIndices,:,:]\n",
        "        yTrainBatch = yTrain[thisBatchIndices]\n",
        "        pTrainBatch = pTrain[thisBatchIndices]\n",
        "        # compute loss and gradients\n",
        "        loss, grads = loss_grad_fn(params, model, xTrainBatch, S, yTrainBatch, pTrainBatch)\n",
        "        # optimizer step\n",
        "        updates, opt_state = tx.update(grads, opt_state)\n",
        "        # update parameters\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        # append loss for plotting\n",
        "        lossTrain.append(loss)\n",
        "        epoch_loss += loss\n",
        "    print(f\"Epoch: {epoch+1} \\t Loss {epoch_loss/nBatches:.2f} \\n\")\n"
      ],
      "metadata": {
        "id": "ybEmL2WffWZH"
      },
      "id": "ybEmL2WffWZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Training error evolution\n",
        "\n",
        "We use matplotlib to visualize the evolution of the mean squared error during training."
      ],
      "metadata": {
        "id": "gHmfxm-7hwMh"
      },
      "id": "gHmfxm-7hwMh"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(lossTrain, label = 'Train MSE') \n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.xlabel('Batches')\n",
        "plt.title('Graph Filter Training Loss Evolution')"
      ],
      "metadata": {
        "id": "O1igj-AKfWb_"
      },
      "id": "O1igj-AKfWb_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate test performance"
      ],
      "metadata": {
        "id": "R_sYYiK8hzB5"
      },
      "id": "R_sYYiK8hzB5"
    },
    {
      "cell_type": "code",
      "source": [
        "yHatTest = model.apply(params, xTest, S)\n",
        "testMSE = movieMSELoss(yHatTest, yTest, pTest)\n",
        "print(f\"Graph Filter Test Mean Squared Error: {testMSE:.5f}\")"
      ],
      "metadata": {
        "id": "PZW2PZr8fpju"
      },
      "id": "PZW2PZr8fpju",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "99372137",
      "metadata": {
        "id": "99372137"
      },
      "source": [
        "<hr>\n",
        "\n",
        "# PART 3 - Graph Neural Networks\n",
        "\n",
        "<hr>\n",
        "\n",
        "Graph neural networks (GNNs) extend graph filters by using pointwise nonlinearities which are nonlinear functions that are applied independently to each component of a vector. For a formal definition, begin by introducing a single variable function $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ which we extend to the matrix function $\\sigma:\\mathbb{R}^{n\\times q}\\to\\mathbb{R}^{n\\times q}$ by independent application to each component. Thus, if we have $\\mathbf{U} \\in \\mathbb{R}^{n\\times q}$ the output matrix $\\sigma(\\mathbf{U})$ is such that\n",
        "\n",
        "\\begin{equation}\n",
        "\\sigma\\big(\\,\\mathbf{U}\\,\\big) \\ : \\ \\big[\\,\\sigma\\big(\\,\\mathbf{U}\\,\\big)\\,\\big]_{i,j} = \\sigma\\big(\\,\\mathbf{U}_{i,j}\\,\\big).\n",
        "\\end{equation}\n",
        "\n",
        "In a single layer GNN, the graph signal $\\mathbf{U} = \\sum_{k=0}^{K-1} \\mathbf{U}_k \\mathbf{H}_k$ is passed through a pointwise nonlinear function to produce the output\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbf{\\Phi}(\\mathbf{X};\\mathcal{H}, \\mathbf{S}) = \\sigma(\\mathbf{U}) = \\sigma \\left( \\sum_{k=0}^{K-1} \\mathbf{S}^k \\mathbf{X} \\mathbf{H}_k \\right) \\ .\n",
        "\\end{equation}\n",
        "\n",
        "We say that the transform above is a graph perceptron. Different from the graph filter, the graph perceptron is a nonlinear function of the input. It is, however, a very simple form of nonlinear processing because the nonlinearity does not mix signal components. Signal components are mixed by the graph filter but are then processed element-wise through $\\sigma$.\n",
        "\n",
        "\n",
        "### Multi-layer GNN\n",
        "\n",
        "Graph perceptrons can be stacked in layers to create multi-layer GNNs. This stacking is mathematically written as a function composition where the outputs of a layer become inputs to the next layer. For a formal definition let $l=1,\\ldots,L$ be a layer index and $\\mathbf{H}_l=\\{H_{l k}\\}_{k=0}^{K-1}$ be collections of $K$ graph filter coefficients associated with each layer. At layer $l$ we take as input the output $\\mathbf{X}_{l-1}$ of layer $l-1$ which we process with the filter $\\mathbf{\\Phi} (\\mathbf{C}; \\mathcal{H}_l, \\mathbf{S})$ to produce the intermediate feature\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{U}_{l} \\ = \\ \\sum_{k=0}^{K-1} \\mathbf{S}^k\\, \\mathbf{X}_{l-1} \\mathbf{H}_{l k},\n",
        "\\end{align}\n",
        "\n",
        "where, by convention, we say that $\\mathbf{X}_0 = \\mathbf{X}$ so that the given graph signal $\\mathbf{X}$ is the GNN input. As in the case of the graph perceptron, this feature is passed through a pointwise nonlinear function to produce the $l$th layer output\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{X}_{l} \\ = \\ \\sigma(\\mathbf{U}_l )\n",
        "\\ = \\ \\sigma\\Bigg(\\sum_{k=0}^{K-1} \\mathbf{S}^k\\, \\mathbf{X}_{l-1} \\mathbf{H}_{l k}\\Bigg) .\n",
        "\\end{align}\n",
        "\n",
        "After recursive repetition of the above formula for $l=1,\\ldots,L$ we reach the $L$th layer whose output $\\mathbf{x}_L$ is not further processed and is declared the GNN output $\\mathbf{y}=\\mathbf{x}_L$. To represent the output of the GNN we define the filter tensor $\\mathcal{H}:=\\{\\mathcal{H}_l\\}_{l=1}^L$ grouping the $L$ tensors of filter coefficients at each layer, and define the operator $\\mathbf{\\Phi}(\\,\\cdot\\,;\\mathcal{H},\\mathbf{S})$ as the map\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{\\Phi} (\\mathbf{X}; \\mathcal{H}, \\mathbf{S}) = \\mathbf{X}_L.\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "Implement a multi-layer GNN can be built using the nn.Module that takes in the number of layers, the number of filter taps in each layer and the non-linearity and instantiates as many graph filters as there are layers. The ``__call__`` recieves the input signal and adjacency matrix, and computes the output of the GNN.\n"
      ],
      "metadata": {
        "id": "XdaitNjPEC04"
      },
      "id": "XdaitNjPEC04"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "295015a2",
      "metadata": {
        "id": "295015a2"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class GNNModule(nn.Module):\n",
        "    l: int # number of layers\n",
        "    k: list #(list of length n) with order of filter for each layer\n",
        "    f: list #(vector of length n+1) input dimension to each layer\n",
        "    sigma: nn.Module # nonlinear activation function (same for all layers)\n",
        "    # setup method gets called on init\n",
        "    def setup(self):\n",
        "        gml = []\n",
        "        ## append layers\n",
        "        #for layer in range(self.l-1):\n",
        "            #gml.append() ## IMPLEMENT ###\n",
        "        self.gml = gml\n",
        "        # add last linear layer\n",
        "        #self.readout = ## IMPLEMENT\n",
        "    \n",
        "    # Define the forward pass\n",
        "    def __call__(self, x, S):\n",
        "        #for layer in self.gml:\n",
        "            ## compute graph filter\n",
        "            #x = ## IMPLEMENT\n",
        "            ## apply non-linearity\n",
        "            #x = ## IMPLEMENT\n",
        "        # linear readout\n",
        "        #x = ## IMPLEMENT\n",
        "        return #x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer"
      ],
      "metadata": {
        "id": "pUPE5kCgET-_"
      },
      "id": "pUPE5kCgET-_"
    },
    {
      "cell_type": "code",
      "source": [
        "class GNNModule(nn.Module):\n",
        "    l: int # number of layers\n",
        "    k: list #(list of length n) with order of filter for each layer\n",
        "    f: list #(vector of length n+1) input dimension to each layer\n",
        "    sigma: nn.Module # nonlinear activation function (same for all layers)\n",
        "    # setup method gets called on init\n",
        "    def setup(self):\n",
        "        gml = []\n",
        "        # append layers\n",
        "        for layer in range(self.l-1):\n",
        "            gml.append(FilterModule(self.k[layer], self.f[layer],self.f[layer+1]))\n",
        "        self.gml = gml\n",
        "        # add last linear layer\n",
        "        self.readout = FilterModule(1,self.f[layer+1], self.f[layer+2])\n",
        "    \n",
        "    # Define the forward pass\n",
        "    def __call__(self, x, S):\n",
        "        for layer in self.gml:\n",
        "            # graph filter\n",
        "            x = layer(x, S)\n",
        "            # non-linearity\n",
        "            x = self.sigma(x)\n",
        "        # linear readout\n",
        "        x = self.readout(x, S)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1cJF-r5TD3D8"
      },
      "id": "1cJF-r5TD3D8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a GNN\n"
      ],
      "metadata": {
        "id": "3O2azvPdJ39-"
      },
      "id": "3O2azvPdJ39-"
    },
    {
      "cell_type": "markdown",
      "id": "b475e6df",
      "metadata": {
        "id": "b475e6df"
      },
      "source": [
        "\n",
        "\n",
        "## Task 6\n",
        "Train a GNN to predict movie ratings. Plot the evolution of the\n",
        "training loss and evaluate the loss in the test dataset. To obtain a good\n",
        "loss we need to experiment with the number of layers and the number of\n",
        "filter taps per layer.\n",
        "\n",
        "We will use the following parameters (also try your own!)\n",
        "\n",
        "*   Number of layers: 3\n",
        "*   Filter order at each layer: 5, 5, 1\n",
        "*   Input dimension at each layer: 1, 64, 32, 1\n",
        "*   Non-linearity: ReLU(x) = max(0, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2152c4",
      "metadata": {
        "id": "9b2152c4"
      },
      "outputs": [],
      "source": [
        "### Define Parameters\n",
        "#l = #number of layers\n",
        "#k = # taps\n",
        "#f = # filter outputs\n",
        "#sigma = # non-linearity\n",
        "### Instantiate GNN model\n",
        "#model = GNNModule(l, k, f, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer"
      ],
      "metadata": {
        "id": "_BJSEqhZJDfH"
      },
      "id": "_BJSEqhZJDfH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Parameters\n",
        "l = 3\n",
        "k = [5,5,1]\n",
        "f = [1, 64, 32, 1]\n",
        "sigma = nn.relu\n",
        "\n",
        "model = GNNModule(l, k, f, sigma)"
      ],
      "metadata": {
        "id": "zDPAibJ1JCBd"
      },
      "id": "zDPAibJ1JCBd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3f5ebcd3",
      "metadata": {
        "id": "3f5ebcd3"
      },
      "source": [
        "### Initialise Model and Optimizer\n",
        "\n",
        "We use the same training parameters. The following cell initialises the model and optimizer. (Run Cell)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3eef585",
      "metadata": {
        "id": "a3eef585"
      },
      "outputs": [],
      "source": [
        "nEpochs = 8\n",
        "learningRate = 0.01\n",
        "batchSize = 20\n",
        "\n",
        "# create a random key for initialization\n",
        "key1 = random.PRNGKey(0)\n",
        "\n",
        "# we need to pass the random key and\n",
        "# data inputs to call to the initialisation\n",
        "# becasue it computes a forward pass\n",
        "params =  model.init(key1, xTrain, S)\n",
        "\n",
        "# optimizer\n",
        "tx = optax.adam(learning_rate=learningRate)\n",
        "opt_state = tx.init(params)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0825a9d7",
      "metadata": {
        "id": "0825a9d7"
      },
      "source": [
        "\n",
        "### Training loop (Run Cell)\n",
        "\n",
        "Below is the main trainig loop implementation. This is a very straightforward implementation based on a fixed number of epochs. \n",
        "For each batch, the following steps are performed:\n",
        "\n",
        "*   Compute prediction\n",
        "*   Evaluate loss\n",
        "*   Compute gradient of loss wrt parameters.\n",
        "*   Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8b3ce76",
      "metadata": {
        "id": "b8b3ce76"
      },
      "outputs": [],
      "source": [
        "epoch = 0 # epoch counter\n",
        "lossTrain = []\n",
        "for epoch in range(nEpochs):\n",
        "    # permute samples\n",
        "    randomPermutation = np.random.permutation(nTrain)\n",
        "    idxEpoch = [int(i) for i in randomPermutation]\n",
        "    epoch_loss = 0.\n",
        "    for batch in tqdm(range(nBatches)):\n",
        "        # Determine batch indices\n",
        "        thisBatchIndices = idxEpoch[batchIndex[batch]: batchIndex[batch+1]]\n",
        "        # Get the samples in this batch\n",
        "        xTrainBatch = xTrain[thisBatchIndices,:,:]\n",
        "        yTrainBatch = yTrain[thisBatchIndices]\n",
        "        pTrainBatch = pTrain[thisBatchIndices]\n",
        "        # compute loss and gradients\n",
        "        loss, grads = loss_grad_fn(params, model, xTrainBatch, S, yTrainBatch, pTrainBatch)\n",
        "        # optimizer step\n",
        "        updates, opt_state = tx.update(grads, opt_state)\n",
        "        # update parameters\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        # append loss for plotting\n",
        "        lossTrain.append(loss)\n",
        "        epoch_loss += loss\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} \\t Loss {epoch_loss/nBatches:.2f} \\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b369b1c",
      "metadata": {
        "id": "9b369b1c"
      },
      "source": [
        "\n",
        "## Training error evolution (Run Cell)\n",
        "\n",
        "We use matplotlib to visualize the evolution of the mean squared error during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d61e1c95",
      "metadata": {
        "id": "d61e1c95"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(lossTrain, label = 'Train MSE') \n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.xlabel('Batches')\n",
        "plt.title('GNN Training Loss Evolution')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab22f87e",
      "metadata": {
        "id": "ab22f87e"
      },
      "source": [
        "\n",
        "## Evaluation\n",
        "\n",
        "We evaluate the testing accuracy for the GNN.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5a7770",
      "metadata": {
        "id": "7c5a7770"
      },
      "outputs": [],
      "source": [
        "yHatTest = model.apply(params, xTest, S)\n",
        "testMSE = movieMSELoss(yHatTest, yTest, pTest)\n",
        "print(f\"GNN Test Mean Squared Error: {testMSE:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feedback**\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ],
      "metadata": {
        "id": "5sEtyQJLLmxm"
      },
      "id": "5sEtyQJLLmxm"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Please run this cell to load the feedback form:\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://forms.gle/AjhUiYxfLFV5gMGa7\",\n",
        "  width=\"80%\"\n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "dMxzLfV3LnzI",
        "cellView": "form"
      },
      "id": "dMxzLfV3LnzI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://github.com/khipu-ai/global-resources/raw/main/logo/khipu.png\" /></center>\n"
      ],
      "metadata": {
        "id": "Aademc62LKzz"
      },
      "id": "Aademc62LKzz"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pP_ko1OI9EWz",
        "sQXGFjWL_lAw",
        "deJCl4slBCu3",
        "pUPE5kCgET-_",
        "_BJSEqhZJDfH"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "d9b28b648a7257234d1b762ad6da44bfd56b062b5eb0597ff56d676ff99f7fd2"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
